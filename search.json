[{"categories":null,"content":"前言 OpenShift 4.X 版本要求安装在操作系统为 CoreOS 的机器上，因此 官方文档 给出了使用 PXE 或 IPXE 引导 CoreOS 系统的方法。我们可以参考其操作流程，将一台 CentOS 7.X 的机器改写为 CoreOS 系统，步骤如下：\n  从 镜像下载页 下载安装所需版本的 kernel、initramfs 和 rootfs 文件，并将 rootfs 和点火文件（*.ign）上传到自建的 HTTP 服务器上；\n  将 kernel 和 initramfs 文件拷贝到 CentOS 7.X 机器的 /boot 目录下；\n  根据需求修改 /boot/grub2 目录下的 grub.cfg 文件；\n  重启机器。\n  对于操作系统初学者（比如我）来说，很难想象仅依靠添加和修改文件就能改变一台计算机的操作系统。为了解其实现原理，我们将对 Linux 的启动流程进行讨论，并从中说明上述操作是如何影响操作系统的。\nLinux 启动流程 启动一台 Linux 机器的过程可以分为两个部分：Boot 和 Startup。其中，Boot 起始于计算机启动，在内核初始化完成且 systemd 进程开始加载后结束。紧接着， Startup 接管任务，使计算机达到一个用户可操作的状态。\nBoot 阶段 如上图所示，Boot 阶段又可以细分为三个部分：\n BIOS POST Boot Loader 内核初始化  BIOS POST 开机自检（Power On Self Test，POST）是 基本输入输出系统（Basic I/O System，BIOS）的一部分，也是启动 Linux 机器的第一个步骤。其工作对象是计算机硬件，因此对于任何操作系统都是相同的。POST 检查硬件的基本可操作性，若失败则 Boot 过程将会被终止。\nPOST 检查完毕后会发出一个 BIOS 中断调用 INT 13H，它将在任何可连接且可引导的磁盘上搜索含有有效引导记录的引导扇区（Boot Sector），通常是 主引导扇区。引导扇区中的主引导记录（Master Boot Record，MBR）将被加载到 RAM 中，然后控制权就会转移到其手中。\nBoot Loader 大多数 Linux 发行版使用三种 Boot Loader 程序：GRUB1、GRUB2 和 LILO，其中 GRUB2 是最新且使用最为广泛的。GRUB2 代表“GRand Unified Bootloader, version 2”，它能够定位操作系统内核并将其加载到内存中。GRUB2 还允许用户选择从几种不同的内核中引导计算机，如果更新的内核版本出现兼容性问题，我们就可以恢复到先前内核版本。\nGRUB1 的引导过程可以分为三个阶段：stage 1、stage 1.5 和 stage 2。虽然 GRUB2 中并没有 stage 的概念，但两者的工作方式基本相同。为了方便说明，我们在讨论 GRUB2 时将沿用 GRUB1 中 stage 的说法。\nstage 1 上文提到，BIOS 中断调用会定位主引导扇区，其结构如下图所示：\n主引导记录首部的引导代码便是 stage 1 文件 boot.img，它和 stage 1.5 文件 core.img 均位于 /boot/grub2/i386-pc 目录下：\n1 2 3  [root@bastion ~]# du -b /boot/grub2/i386-pc/*.img  512 /boot/grub2/i386-pc/boot.img 26664 /boot/grub2/i386-pc/core.img   它的作用是检查分区表是否正确，然后定位和加载 stage 1.5 文件。446 字节的 boot.img 放不下能够识别文件系统的代码，只能通过计算扇区的偏移量来寻找，因此 core.img 必须位于主引导记录和驱动器的第一个分区（partition）之间。第一个分区从扇区 63 开始，与位于扇区 0 的主引导记录之间有 62 个扇区（每个 512 字节），有足够的空间存储大小不足 30000 字节的 core.img 文件。当 core.img 文件加载到 RAM 后，控制权也随之转移。\nstage 1.5 相比于只能读取原始扇区的 LILO，GRUB1 和 GRUB2 均可识别文件系统，这依赖于 stage 1.5 文件中内置的文件系统驱动程序。如果你拥有一台仍然使用 GRUB1 引导的 CentOS 6.X 机器，那么便可以在 /boot/grub/ 目录下找到这些适配不同文件系统的 stage 1.5 文件：\n1 2 3 4 5 6 7 8 9 10 11  [root@centos6.5 ~]# du -b /boot/grub/* | grep stage1_5 13380 /boot/grub/e2fs_stage1_5 12620 /boot/grub/fat_stage1_5 11748 /boot/grub/ffs_stage1_5 11756 /boot/grub/iso9660_stage1_5 13268 /boot/grub/jfs_stage1_5 11956 /boot/grub/minix_stage1_5 14412 /boot/grub/reiserfs_stage1_5 12024 /boot/grub/ufs2_stage1_5 11364 /boot/grub/vstafs_stage1_5 13964 /boot/grub/xfs_stage1_5   GRUB2 中的 core.img 不仅整合了上述文件系统驱动，还新增了菜单处理等模块，这也是其优于 GRUB1 的地方。我们可以在 GNU GRUB Manual 2.06: Images 中找到对各种 GRUB 镜像文件的详细介绍。\n既然 core.img 文件可以识别文件系统，那么它就能够根据安装时确定的系统路径定位和加载 stage 2 文件。同样，当 stage 2 文件加载到 RAM 后，控制权也随之转移。\nstage 2 stage 2 文件并非是一个 .img 的镜像，而是一些运行时内核模块：\n1 2 3 4 5 6 7 8 9 10 11  [root@bastion ~]# ls /boot/grub2/i386-pc/ | grep .mod | head acpi.mod adler32.mod affs.mod afs.mod ahci.mod all_video.mod aout.mod appendedsig.mod appended_signature_test.mod archelp.mod   它们的任务是根据 grub.cfg 文件的配置定位和加载内核文件，然后将控制权转交给 Linux 内核。grub.cfg 文件存放在 /boot/grub2 目录下：\n1 2 3 4 5 6  [root@bastion ~]# head /boot/grub2/grub.cfg -n 5 # # DO NOT EDIT THIS FILE # # It is automatically generated by grub2-mkconfig using templates # from /etc/grub.d and settings from /etc/default/grub   通过该文件的注释我们可以知道，它实际上是由 grub2-mkconfig 命令使用 /etc/grub.d 目录下的一些模板文件并根据 /etc/default/grub 文件中的设置生成的：\n1 2  [root@bastion ~]# ls /etc/grub.d/ 00_header 00_tuned 01_users 10_linux 20_linux_xen 20_ppc_terminfo 30_os-prober 40_custom 41_custom README   40_custom 和 41_custom 文件常用于用户对 GRUB2 配置的修改，实际上我们对机器的操作也是从这里开始的。为了让 GRUB2 在机器启动时选择 CoreOS 系统内核而非默认的 CentOS，需要在原始 40_custom 文件末尾添加如下内容：\n1 2 3 4 5  menuentry 'coreos' { set root='hd0,msdos1' linux16 /rhcos-live-kernel-x86_64 coreos.inst=yes coreos.inst.install_dev=vda rd.neednet=1 console=tty0 console=ttyS0 coreos.live.rootfs_url=http://{{HTTP-Server-Path}}/rhcos-live-rootfs.x86_64.img coreos.inst.ignition_url=http://{{HTTP-Server-Path}}/master.ign ip=dhcp initrd16 /rhcos-live-initramfs.x86_64.img }   所示的 Menuentry 由三条 Shell 命令组成：\n set root='hd0,msdos1' linux16 /rhcos-live-kernel-x86_64 ... initrd16 /rhcos-live-initramfs.x86_64.img  第一条命令指定了 GRUB2 的根目录，也就是 /boot 所在分区在计算机硬件上的位置。既然我们已经将内核文件拷贝到了 /boot 目录下，那么能够识别文件系统的 GRUB2 便可以定位和加载它。本例中hd代表硬盘（hard drive），0代表第一块硬盘，mosdos代表分区格式，1 代表第一个分区。详细的硬件命名规范见 Naming Convention。\n第二条命令将从rhcos-live-kernel-x86_64（CoreOS 系统的内核文件）中以 16 位模式加载 Linux 内核映像，并通过coreos.live.rootfs_url和coreos.inst.ignition_url参数指定根文件系统（rootfs）的镜像文件和点火文件的下载链接。ip=dhcp代表该计算机网络将由 DHCP 服务器动态配置，也可以按ip={{HostIP}}::{{Gateway}}:{{Genmask}}:{{Hostname}}::none nameserver={{DNSServer}}的格式写入静态配置。\n第三条命令将从rhcos-live-initramfs.x86_64.img中加载 RAM Filesystem。GRUB2 读取的内核文件实际上只包含了内核的核心模块，缺少硬件驱动模块的它无法完成 rootfs 的挂载。然而这些硬件驱动模块位于 /lib/modules/$(uname -r)/kernel/ 目录下，必须在 rootfs 挂载完毕后才能被识别和加载。为了解决这一问题，initramfs（前身为 initrd）应运而生。它是一个包含了必要驱动模块的临时 rootfs，内核可以从中加载所需的驱动程序。待真正的 rootfs 挂载完毕后，它便会从内存中移除。\n除此之外我们还需要将 /etc/default/grub 文件中的 GRUB_DEFAULT=saved 修改为 GRUB_DEFAULT=“coreos”，使其与 40_custom 文件中的menuentry 'coreos'对应。最后使用命令grub2-mkconfig -o /boot/grub2/grub.cfg来重新生成一份 grub.cfg 文件，这样计算机重启后 GRUB2 就会根据我们的配置去加载 CoreOS 系统的内核了。\n至此我们已经明白了为什么“仅依靠添加和修改文件就能改变一台计算机的操作系统”，但计算机想要达到用户可操作状态还远不止于此。让我们再来看看内核被加载到内存后发生了什么。\n内核初始化 不同内核及其相关文件位于 /boot 目录中，均以 vmlinuz 开头：\n1 2 3 4  [root@bastion ~]# ls /boot/ | grep vmlinuz vmlinuz-0-rescue-20210623110808105647395700239158 vmlinuz-4.18.0-305.12.1.el8_4.x86_64 vmlinuz-4.18.0-305.3.1.el8.x86_64   内核通过压缩自身来节省存储空间，所以当选定的内核被加载到内存中后，它首先需要进行解压缩（extracting）。一旦解压完成，内核便会开始加载 systemd 并将控制权移交给它。\nStartup 阶段 systemd 是所有进程之父，它负责使计算机达到可以完成生产工作的状态。其功能比过去的 init 程序要丰富得多，包括挂载文件系统、启动和管理计算机所需的系统服务。当然你也可以将一些应用（如 Docker）以 systemd 的方式启动，但它们与 Linux 的启动无关，因此不在本文的讨论范围之内。\n首先，systemd 根据 /etc/fstab 文件中的配置挂载文件系统。然后读取 /etc 目录下的配置文件，包括其自身的配置文件 /etc/systemd/system/default.target。该文件指定了 systemd 需要引导计算机到达的最终目标和状态，实际上是一个软链接：\n1 2  [root@bastion ~]# ls /etc/systemd/system/default.target -l lrwxrwxrwx. 1 root root 37 Oct 17 2019 /etc/systemd/system/default.target -\u003e /lib/systemd/system/multi-user.target   在我使用的 bastion 服务器上，它指向的是 multi-user.target；对于带有图形化界面的桌面工作站，它通常指向 graphics.target；而对于单用户模式的机器，它将指向 emergency.target。target 等效于过去 SystemV 中的 运行级别（Runlevel），它提供了别名以实现向后兼容性：\n   SystemV Runlevel systemd target systemd target alias Description      halt.target  在不关闭电源的情况下中止系统。   0 poweroff.target runlevel0.target 中止系统并关闭电源。   s emergency.target  单用户模式。 没有服务正在运行，也未挂载文件系统。仅在主控制台上运行一个紧急 Shell，供用户与系统交互。   1 rescue.target runlevel1.target 一个基本系统。文件系统已挂载，只运行最基本的服务和主控制台上的紧急 Shell。   2  runlevel2.target 多用户模式。虽然还没有网络连接，但不依赖网络的所有非 GUI 服务都已运行。   3 multi-user.target runlevel3.target 所有服务都在运行，但只能使用命令行界面（CLI）。   4  runlevel4.target 用户自定义   5 graphical.target runlevel5.target 所有服务都在运行，并且可以使用图形化界面（GUI）。   6 reboot.target runlevel6.target 重启系统    每个 target 都在其配置文件中指定了一组依赖，由 systemd 负责启动。这些依赖是 Linux 达到某个运行级别所必须的服务（service）。换句话说，当一个 target 配置文件中的所有 service 都已成功加载，那么系统就达到了该 target 对应的运行级别。\n下图展示了 systemd 启动过程中各 target 和 service 实现的一般顺序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  cryptsetup-pre.target veritysetup-pre.target | (various low-level v API VFS mounts: (various cryptsetup/veritysetup devices...) mqueue, configfs, | | debugfs, ...) v | | cryptsetup.target | | (various swap | | remote-fs-pre.target | devices...) | | | | | | | | | v | v local-fs-pre.target | | | (network file systems) | swap.target | | v v | | | v | remote-cryptsetup.target | | | (various low-level (various mounts and | remote-veritysetup.target | | | services: udevd, fsck services...) | | remote-fs.target | | tmpfiles, random | | | / | | seed, sysctl, ...) v | | / | | | local-fs.target | | / | | | | | | / \\____|______|_______________ ______|___________/ | / \\ / | / v | / sysinit.target | / | | / ______________________/|\\_____________________ | / / | | | \\  | / | | | | | | / v v | v | | / (various (various | (various | |/ timers...) paths...) | sockets...) | | | | | | | | v v | v | | timers.target paths.target | sockets.target | | | | | | v | v \\_______ | _____/ rescue.service | \\|/ | | v v | basic.target rescue.target | | | ________v____________________ | / | \\  | | | | | v v v | display- (various system (various system | manager.service services services) | | required for | | | graphical UIs) v v | | multi-user.target emergency.service | | | | \\_____________ | _____________/ v \\|/ emergency.target v graphical.target   如上图所示，想要到达到某个 target，其依赖的所有 target 和 service 就必须已完成加载。如实现 sysinit.target，需要先挂载文件系统（local-fs.target）、设置交换文件（swap.target）、初始化 udev （various low-level services）和设置加密服务（cryptsetup.target）等。不过，同一个 target 的不同依赖项可以并行执行。\n当计算机达到 multi-user.target 或 graphical.target 时，它的漫漫启动之路就走到了尽头。但为了满足用户多样的需求，它所面临的挑战其实才刚刚开始。\nFuture Work  前言提到 RedHat 官方给出了 IPXE/PXE 引导 CoreOS 系统的方法，那么这项技术又是如何实现的呢？ MBR 只有 446 个字节，可为什么 boot.img 文件却有 512 个字节？ 目前已经有越来越多的计算机使用 UEFI 和 GPT 来代替 BIOS 和 MBR，其优势体现在哪？ 我们该如何理解 systemd 的配置文件？如何使用 systemd 部署我们的应用？  参考文献 Creating Red Hat Enterprise Linux CoreOS (RHCOS) machines by PXE or iPXE Booting\nBIOS - Wikipedia\nINT 13H - Wikipedia\n主引导记录 - Wikipedia\nAn Introduction To the Linux Boot and Startup Processes\nGNU GRUB Manual 2.06\nBootup(7) - Linux manual page\n","description":"","tags":["OS","Linux","CoreOs"],"title":"通过安装 CoreOS 系统了解 Linux 启动流程","uri":"/posts/understand-linux-boot-process-by-installing-coreos/"},{"categories":null,"content":"在使用高级语言，如 C、Java 编程时，我们无法了解程序具体的机器级实现。相比之下，使用汇编语言编写程序时，程序员必须指定程序使用的低级指令来执行计算。编译器提供的类型检查有助于检测许多程序错误，确保我们以一致的方式来引用和操作数据。最重要的是，用高级语言编写的程序可以在多种不同的机器上编译运行，而汇编语言则与机器特性高度相关。\n尽管编译器完成了生成汇编代码的大部分工作，但阅读和理解汇编语言对于程序员来说是一项重要的技能：\n Those who say “I understand the general principles, I don’t want to bother learning the details” are deluding themselves.\n Intel 处理器历史 程序编码 机器级代码 首先，机器级程序的格式和行为是由指令集架构（instruction set architecture，ISA）定义的，包括处理器状态、指令格式以及每条指令对状态的影响。大多数 ISA，包括 x86-64，都将程序的行为描述为每条指令按顺序执行，且一条指令在下一条指令开始之前完成。虽然处理器硬件要复杂得多，可以同时执行许多指令，但它采用了安全措施来确保其整体行为与 ISA 规定的操作顺序相匹配。其次，机器级程序使用的内存地址是虚拟地址，提供了一个看似非常大的字节数组的内存模型。\n汇编代码表示非常接近机器代码，与机器代码的二进制格式相比，它采用更具可读性的文本格式。一些对程序员隐藏的处理器状态在汇编代码中是可见的：\n 程序计数器（PC）：在 x86-64 中称为 %rip，代表即将执行的下一条指令在存储器中的地址； 包含 16 个位置（location）的整数寄存器文件（register file）：这些位置均被命名，每个都能存储 64 位的值。该寄存器可以保存地址（与 C 中的指针对应）和整数数据。一些寄存器用于记录程序状态的关键部分，而其他寄存器则用于保存临时数据，例如过程中的参数、局部变量和函数返回值； 条件码寄存器（condition code registers）：保存了最新执行的算术或逻辑指令的状态信息，用于实现控制流或数据流中条件的改变，例如 if 语句和 while 语句； 一组向量寄存器（vector registers）：每个都可以保存一个或多个整数或浮点数值。  虽然 C 提供了一个模型，让我们可以在内存中声明和分配不同数据类型的对象。但机器级代码只会简单地将内存视为一个按字节寻址的数组，因此 C 中的聚合数据类型（如数组和结构体）在机器级代码中会表示为连续的字节集合。甚至对于标量数据类型（如 int、char、float 和 bool 等），汇编代码也不会区分有符号和无符号整数、不同类型的指针以及指针和整数。\n程序的可执行机器级代码、操作系统所需的一些信息、用于管理过程调用和返回（procedure calls and returns）的运行时栈以及用户分配的内存块（如使用 malloc 库函数）共同构成了程序内存，它使用虚拟地址寻址，不过只有部分虚拟地址的范围有效。例如，x86-64 机器的虚拟地址必须将前 16 位设置为 0，因此其有效范围包含 $2^{48}$ （64 TB）字节。操作系统负责管理该虚拟地址空间，并将其转换为实际处理器内存（processer memory）中的物理地址。\n单个机器指令仅执行一些基本的操作，如将存储在寄存器中的两个数字相加、在内存和寄存器之间传输数据以及有条件地跳转到新的指令地址。编译器必须生成这样的指令序列来实现程序结构，例如算术表达式求值、循环或过程调用和返回。\n代码示例 C 程序文件 mstore.c 中包含如下代码：\n1 2 3 4 5  long mult2(long, long); void multstore(long x, long y, long *dest) { long t = mult2(x, y); *dest = t; }   使用gcc -Og -S mstore.c命令即可生成汇编代码文件 mstore.s，其中的-Og代表对代码进行优化。汇编代码中有多种声明，包括：\n每一行缩进的代码都对应着一条机器指令，图中的蓝色注解则标示了指令的作用，如pushq代表将寄存器 %rbx 的内容压入程序栈中。原程序中局部变量名称以及数据类型的所有信息都已被删除，随后我们可以在 Linux 系统中执行下列命令：\n1 2  gcc -Og -c mstore.c objdump -d mstore.o   第一条命令将生成二进制格式的目标代码文件（object-code file）mstore.o，第二条命令则是进行反汇编，即将机器级代码转换为一种与汇编语言格式类似的代码。图中的行号和斜体注释是为了方便说明加入的，左侧有 6 组十六进制字节序列，每个都是一条指令，右侧则显示了等效的汇编代码：\n x86-64 指令的长度范围为 1 到 15 个字节，常用的和操作数较少的指令比不太常用或操作数较多的指令需要更少的字节数； 从给定的起始位置开始，将字节唯一地解码为机器指令。例如，只有指令 pushq %rbx 以字节值 53 开头； 反汇编程序只是根据机器级代码文件中的字节序列来确定汇编代码，不需要访问源代码或汇编代码； 反汇编程序使用的指令命名规则与 gcc 生成的汇编代码略有不同，如许多指令中的后缀q被省略了。这些后缀是尺寸指示符，在大多数情况下可以省略。而反汇编器在call和ret指令中添加了后缀q，它们同样是可以省略的。  想要生成实际的可执行代码还需要在一组包含main函数的目标代码文件上运行链接器（linker），假设 main.c 文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12  #include \u003cstdio.h\u003evoid multstore(long, long, long *); int main() { long d; multstore(2, 3, \u0026d); printf(\"2 * 3 --\u003e %ld\\n\", d); return 0; } long mult2(long a, long b) { long s = a * b; return s; }   那么通过gcc -Og -o prog main.c mstore.c命令生成的可执行文件 prog 的大小就超过了 mstore.o，因为它还包含了用于启动和终止程序以及与操作系统交互的代码。同样对 prog 文件使用objdump命令进行反汇编，其输出结果包含如下代码：\n与第一次反汇编的结果相比，区别主要在：\n 链接器将代码的地址（Offset）移动到了不同的地址范围内，因此左侧的地址不同； 链接器的一项任务是将函数调用与这些函数的可执行代码的位置进行匹配。因此在结果第 4 行，链接器填充了callq指令在调用函数mult2时应该使用的地址； 结果第 8，9 行填充的代码对结果没有任何影响，nop 意为 no operation。插入它们是为了将函数的代码增加到 16 字节，这样可以更好地放置下一个代码块，从而提升存储器的系统性能。  数据格式 Intel 用术语“字”（word）来表示 16 位数据类型，因此 32 位数称为“双字”（double words），64 位数称为“四字”（quad words）。在 x86-64 机器上 C 的原始数据类型大小如下：\n   C declaration Intel data type Assembly-code suffix Size(bytes)     char Byte b 1   short Word w 2   int Double word l 4   long Quad word q 8   char * Quad word q 8   float Single precision s 4   double Double precision l 8    大多数由 gcc 生成的汇编代码都有一个表示操作数大小的单字符后缀，如数据移动指令有四种变体：movb（移动字节）、movw（移动字）、movl（移动双字）和movq（移动四字）。值得一提是，后缀“l”即可以表示 4 字节整数又表示 8 字节双精度浮点数。这并不会引起歧义，因为涉及到浮点数的代码使用一组与整数完全不同的指令和寄存器。\n访问信息 上文提到，x86-64 机器的 CPU 中包含 16 个通用寄存器（general-purpose registers），均可以存储 64 位的整数或指针数据：\n图中所有寄存器的名称均以 %r 开头，它们的演变顺序是从右往左的。前 8 个寄存器，即 %ax 到 %sp，是最初的 8086 机器使用的 8 个 16 位寄存器。随着 IA32 的出现，它们被扩展位 32 位，即 %eax 到 %esp。目前的 x86-64 机器将其进一步地扩展为 64 位，即 %rax 到 %rsp。同时还新添加了 8 个寄存器，即 %r8 到 %r15。图中右侧的注释说明了各个寄存器在典型程序中扮演的角色，其中最独特的是栈指针 %rsp，它用于指示运行时栈的结束位置。\n指令可以对存储在寄存器低位字节中的不同大小的数据进行操作。字节级操作可以访问最低有效字节，16 位操作可以访问最低有效 2 个字节，32 位操作可以访问最低有效 4 个字节，64 位操作则可以访问整个寄存器。\n操作数 大多数指令都有一个或多个操作数（operand），指定了执行操作时使用的源数据和结果放置的位置。x86-64 机器支持的操作数格式如下：\n源数据既可以以常数形式给出，也可以从寄存器或内存中读取，结果则存储在寄存器或内存中。因此操作数有三种可能的类型：\n 立即数（immediate）：即常数值，书写方式为 $ 符号后面跟一个整数； 寄存器（register）：寄存器中的内容。我们使用 $r_a$ 表示任意寄存器 $a$，使用 $R[r_a]$ 来表示它的值； 内存（memory）引用：根据计算出的地址（称为有效地址）来访问某个内存位置。使用 $M_b[Addr]$ 表示在内存中从地址 $Addr$ 开始 $b$ 字节的引用，下角标 $b$ 可以省略。  最通用的内存引用方式在上表底部：$M[Imm + R[r_b] + R[r_i]\\times s]$，常用于引用数组元素。\n假设下列值存储在内存或寄存器中指定的地址处：\n那么以下操作数存储的值分别为：\n %rax：0x100 0x104：地址为 0x104，值为 0xAB $0x108：0x108（立即数） (%rax)：地址为 0x100，值为 0xFF 9(%rax,%rdx)：地址为 9 + 0x100 + 0x3 = 0x10C，值为 0x11 260(%rcx,%rdx)：260 即十六进制数 0x104，因此地址为 0x104 + 0x1 + 0x3 = 0x108，值为 0x13 0xFC(,%rcx,4)：地址为 0xFC + 0x1 * 4 = 0x100，值为 0xFF %rax,%rdx,4：地址为：0x100 + 0x3 * 4 = 0x10C，值为 0x11  数据移动指令 在所有机器指令中使用最为频繁的便是数据移动指令，它们负责将数据从一处移动到另一处。我们将操作相同但操作数尺寸不同的指令划分为同一个指令类（instruction classes），下表列出的便是 MOV 指令类中的各种操作：\n上表中的 S 代表 源地址（Source），D 代表目的地址（Destination），I 代表立即数（Immediate），R 代表寄存器（Register）。其中，移动指令不能将一个位于内存中的数据直接移动到内存中的另一个位置，必须经过一个寄存器中转。另外，当movl指令的目的地址为一个寄存器时，它不仅会把目的寄存器的较低四位更新为源数据，还会将较高四位的字节全部置 0。最后，movabsq指令只能将寄存器作为数据的目的地址（R \u003c- I）。下面的例子中，左边为顺序执行的移动指令，右边为寄存器 %rax 中字节的变化情况：\nmovabsq $0x0011223344556677, %rax %rax = 0011223344556677 movb $-1, %al %rax = 00112233445566FF movw $-1, %ax %rax = 001122334455FFFF movl $-1, %eax %rax = 00000000FFFFFFFF movq $-1, %rax %rax = FFFFFFFFFFFFFFFF 还有两类数据移动指令可以将较小尺寸的源数据移动到较大尺寸的目的寄存器，如下表所示：\n两者不同的是，movz指令将目的寄存器的剩余字节均填充为 0（零扩展），movs指令则将其填充为源操作数的最高有效位（符号扩展）。相比于符号扩展，零扩展缺少了指令movzlq。这是因为上文提到，使用movl指令移动数据到寄存器时，会将高位全部置为 0，其效果与零扩展无异。另外，符号扩展还多了一个指令cltq。它没有操作数，且实际上等效于movslq %eax, %rax。\n在 C 中，引用指针（dereference pointer，*p）代表将指针拷贝到寄存器中，然后使用它作为内存地址的引用。而局部变量常存放在寄存器而非内存中，因为这样读取值时更加快速。如下面的一个简单的 C 程序：\n1 2 3 4 5 6  long exchange(long *xp, long y) { long x = *xp; *xp = y; return x; }   与之等效的汇编代码如下：\n; xp in %rdi, y in %rsi exchange: movq (%rdi), %rax movq %rsi, (%rdi) ret 最后的两个数据移动指令是用来将数据向程序栈（Stack）中推入（Push）和从程序栈中推出（Pop），如下表所示：\n在 x86-64 中，程序栈存储在内存中的某些区域中，其特性为后进先出（last-in, first-out）。习惯上我们将栈顶画在底端，而栈顶元素的地址（由栈指针 %rsp 保存）是整个栈中最小的：\n如上图所示，想要将一个四字数据推入栈中，首先要把栈指针减 8，然后令源数据值成为新的栈顶元素。因此指令pushq %rax就等效于：\n; subq 为减法运算，下一章会进行介绍 subq $8, \u0026rsp movq %rax, (%rsp) 同样，若想将栈顶的四字数据推出栈，首先要把栈顶元素的值拷贝到寄存器中，然后再把栈指针加 8。因此指令popq %rdx就等效于：\nmovq (%rsp), %rdx ; addq 为加法运算，下一章会进行介绍 addq $8, %rsp 算术和逻辑操作 下图列出了一些 x86-64 中的整数算数和逻辑操作，其中除了leaq外给出的都是指令类：\n上述操作可分为四类：加载有效地址（load effective address）、一元（unary）、二元（binary）和移位（shifts）。一元操作只有一个操作数，而二元操作则有两个。接下来我们将分别介绍它们。\n加载有效地址 加载有效地址的指令名为leaq，其实质是movq指令的一种变体。它会从内存中读取源操作数的地址拷贝到寄存器中，有时候也实现一些简单的运算。如寄存器 %rdx 中存储的值为 x，那么指令leaq 7(%rdx, %rdx, 4), %rax的作用便是将寄存器 %rax 的值设为 5x+7。\n一元和二元操作 一元操作只有一个操作数，因此源地址和目的地址相同。如操作incq (%rsp)可以将程序栈顶增加 8 个字节的元素，类似于 C 中的自增运算符++。\n二元操作类似于 C 中的赋值运算符（assignment operator），如 x -= y。举例来说，操作subq %rax, %rdx会将寄存器 %rdx 中的值减去寄存器 %rax 中的值。第一个操作数可以是立即数、寄存器或内存中的位置，第二个操作数则只能是寄存器或内存中的位置。参考MOV类指令，二元操作的两个操作数不能同时为内存中的位置。\n移位操作 移位操作的第一个操作符为位数，可以是立即数，也可以是单字节的寄存器（通常使用寄存器 %cl）。第二个操作数为移位的值。可以是寄存器或内存中的位置。对于右移运算来说，sar代表算术右移，shr则代表逻辑右移。\n特殊算数操作 两个 64 位整型的积需要用 128 位来表示，因此 Intel 引入了 16 字节单位“八字”（ oct word）来解决这一问题。下表展示了一些支持运算结果为“八字”的操作：\n我们在普通算数操作和特殊算数操作中均发现了imulq指令。第一种属于 IMUL类，有两个操作数。其计算结果为 64 位（若超过 64 位，则截断高位），等效于第二章介绍的 [无符号乘法](/posts/representing-and-manipulating-information-note/#无符号乘法） 和 二进制补码乘法。第二种即是上表中的特殊算数操作，只有一个操作数，因此编译器可以根据操作数的数量区分它们。\n用于有符号乘法的操作称为imulq，用于无符号乘法的为mulq。两者的第一个参数为寄存器 %rax，第二个参数为源操作数。计算结果中较高 64 位将存储在寄存器 %rdx 中，较低 64 位则存储在寄存器 %rax 中。\n1 2 3 4 5  #include \u003cinttypes.h\u003etypedef unsigned __int128 uint128_t void store_uprod(uint128_t *dest, uint64_t x, uint64_t y){ *dest = x * (uint128_t)y; }   所示的 C 程序在小端（little-endian）机器上可转化为如下的汇编代码，结果中的较高位存储在高地址处 8(%rdi)：\n; dest in %rdi, x in %rsi, y in %rdx store_uprod movq %rsi, %rax mulq %rdx movq %rax, (%rdi) movq %rdx, 8(%rdi) 普通算数操作中没有提供除法或余数运算，因此需要使用特殊算数操作idivq和divq。与乘法类似，被除数的较高 64 位将存储在寄存器 %rdx 中，较低 64 位存储在寄存器 %rax 中。除数为操作数，商保存在寄存器 %rax 中，余数则保存在寄存器 %rdx 中。如果被除数只有 64 位，那么就应当将寄存器 %rdx 全部置为 0（无符号运算）或符号位（有符号运算）。后者可以使用cqto操作实现，它会从寄存器 %rax 中读取符号位，然后将其拷贝到寄存器 %rdx 的每一位中。\n控制 上文中介绍的操作只考虑了顺序执行的代码，对于 C 中的 if、for、while 和 switch 语句，它们需要根据数据检验的结果来决定代码执行的顺序。机器级代码提供了两种基本机制来实现这种条件行为（conditional behavior），一种根据检验结果改变控制流（control flow），另一种则改变数据流（data flow）\n条件码 CPU 维护了一组单字节的条件码（condition code）寄存器，它们记录了最近一次算数或逻辑操作结果的某些属性：\n CF（carry flag）：进位标识，记录最高有效位是否发生进位，用于检测无符号数操作的溢出； ZF（zero flag）：零标识，记录结果是否为 0； SF（sign flag）：符号标识，记录结果是否为负值； OF（overflow flag）：溢出标识，记录是否发生二进制补码溢出。  下列两个指令类可以在不改变任何其他寄存器的情况下设置条件码，如指令testq %rax, %rax可以检测寄存器 %rax 存储的值是正数、负数还是零：\n相比于直接读取，我们更常用以下三种方式使用条件码：\n 根据条件码的组合将单个字节设置为 0 或 1； 有条件地跳转到程序的其他部分； 有条件地传输数据。  下列操作指令便可以实现上述第一种方式。注意，此处的指令后缀代表的并非是不同的操作数大小，而是不同的条件判断。如指令setl和setb分别代表 set less 和 set below：\n下面示例的 C 程序中，首先比较了变量 a 和 b 的大小，然后根据结果把寄存器 %eax 的最低字节（即寄存器 %al）置为 0 或 1。最后一条指令的作用是将寄存器 %eax 的高位三个字节以及寄存器 %rax 的高位四个字节全部清零：\n; int comp(data_t a, data_t b) ; a in rdi%, b in rsi% comp: cmpq %rsi, %rdi setl %al movzbl %al, %eax ret 跳转指令 跳转指令可以让程序转到一个全新的位置继续执行，该位置在汇编代码中使用标签（label）来指定。\nmovq $0, %rax jmp .L1 movq (rax%), %rdx .L1: popq %rdx 指令jmp .L1将使程序跳过movq指令，开始执行popq操作。 下图展示了不同的跳转指令：\n​\njmp指令既可以是直接跳转，也可以是间接跳转。直接跳转的目标使用标签指定，而间接跳转的目标则需要从寄存器或内存中读取。其余的指令均为条件跳转，它们根据条件判断的结果来决定是否执行跳转操作。注意，条件跳转均为直接跳转。\n在生成机器代码的过程中，汇编器和链接器（linker）会确定跳转目标（即目标指令的地址），并编码为跳转指令的一部分。其使用的编码方式有多种，但大多数与程序计数器（PC）相关，即比较目标指令的地址和紧挨着跳转指令的下一条指令的地址之间的差异。这样的说法有些绕口，我们以一个简单的例子来说明：\nmovq %rdi%, %rax jmp .L2 .L3 sarq %rax .L2 testq %rax, %rax jq .L3 rep; ret 该汇编代码包含了两个跳转指令，第一个跳转到了更高的地址处，第二个则相反。而下图是对上述代码汇编然后再进行反汇编后的结果：\n在右侧注释中，第一个跳转指令为 +0x8，第二个跳转指令为 +0x5。再看左侧指令的字节编码，第一个指令的目标被编码为 0x03，将其加上下一条指令的地址 0x5，就得到了跳转目标指令的地址，即第四行指令的地址 0x8。同样，第二个指令的目标被编码为 0xf8，将其加上下一条指令的地址 0xd，就得到了第三行指令的地址 0x3。\n当目标代码文件经过链接器处理后，这些指令会被重新分配地址。不过第二行和第五行跳转目标的编码依然不变，这种方式能够让指令编码更加紧凑：\n使用条件控制实现条件分支 若想将 C 中的条件表达式转化为机器代码，通常使用条件跳转和无条件跳转的组合。一个简单的 C 程序及其编译得到的汇编代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  long lt_cnt = 0; long ge_cnt = 0; long absdiff_se(long x, long y) { long result; if (x \u003c y) { lt_cnt++; result = y - x; } else { ge_cnt++; result = x - y; } }   ; long absdiff_se(long x, long y) ; x in %rdi, y in %rsi absdiff_se: cmpq %rsi, %rdi jge .L2 addq $1, lt_cnt(%rip) movq %rsi, %rax subq %rdi, %rax ret .L2 addq $1, ge_cnt(%rip) movq %rdi, %rax subq %rsi, %rax ret 实际上该汇编代码的控制流（control flow）更像是使用 goto 语句将示例 C 程序改写后得到的结果，即先比较两数大小，然后根据结果决定是否跳转：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  long lt_cnt = 0; long ge_cnt = 0; long gotodiff_se(long x, long y) { long result; if (x \u003e= y) goto x_ge_y; lt_cnt++; result = y - x; return result; x_ge_y: ge_cnt++; result = x - y; return result; }   让我们推广到一般情况。假设 C 中的 if-else 语句模板为：\n1 2 3 4  if (test-expr) then-statement else else-statement   那么编译生成的汇编代码的控制流便可以用如下 C 程序描述：\n1 2 3 4 5 6 7 8  t = test-expr; if (!t) goto false; then-statement goto done; false: else-statement done:   使用条件移动实现条件分支 ","description":"","tags":["CSAPP","OS"],"title":"CSAPP 读书笔记：程序的机器级表示","uri":"/posts/machine-level-representation-of-programs-note/"},{"categories":null,"content":"信息的存储 大多数机器使用字节（8 位的块）作为存储器中的最小寻址单元，而非访问单独的位。内存中的每一个字节都对应一个唯一的数字，即它的地址，所有可能的地址集合构成了 虚拟内存。它是由 DRAM、闪存（flash memory) 和磁盘存储共同实现的，而在程序看来则只是一个统一的字节数组。\n编译器和运行时系统负责将这个内存空间划分为更加可管理的单元，来存储不同的程序对象。例如，C 中指针的值代表存储块中第一个字节的虚拟地址。C 编译器还将每个指针与其类型信息联系起来，这样就可以根据指针值的类型生成不同的机器级代码来访问指针所指向的值。不过机器级代码中并没有任何有关类型的信息，而是简单的把每个程序对象都视为一个字节块。\n十六进制表示法 使用二进制表示位模式（bit pattern）会非常冗长，因为一个字节就包含了 8 位。而如果使用十进制，则不方便与位模式进行互相转化，因此我们采用十六进制（Hexadecimal）来书写位模式。一个十六进制数占 4 位，因此一个字节的取值范围就是 $00_{16}$ ~ $FF_{16}$ 。\n将一个二进制数字转化为十六进制数字，需要首先将其分为多个 4 位的组，然后再将每组数转化为十六进制。如果总位数不为 4 的倍数，那么最左边的一组可以少于四位，然后在首位补 0。如 $111100_2$ 可以分成 $0011_2$ 和 $1100_2$，转化结果为 $3C_{16}$ 。在 C 中，若一个常数以 0x 或 0X 作为前缀，则代表它是一个十六进制数字。\n数据大小 每台计算机都有一个字长（word size），它指定了指针数据的标准大小。如果一台机器的字长为 $w$ 位，那么虚拟地址的范围为 $ 0～2^w -1 $，程序最多访问 $2^w$ 字节。32 位机器的虚拟地址大小约为 4GB，而 64 位机器则能达到 16EB。\nC 中几个基本数据类型的大小如下表所示：\n   Signed Unsigned 32-bit 64-bit     [signed] char unsigned char 1 1   short unsigned short 2 2   int unsigned 4 4   long unsigned long 4 8   int32_t uint_32t 4 4   int64_t uint_64t 8 8   char *  4 8   float  4 4   double  8 8    除 char 外，若不添加前缀 unsigned，则默认使用有符号类型。指针类型的数据使用机器的全字长，如 char *。由于某些数据类型的大小在不同机器上有所不同，因此开发人员应使程序对不同数据类型的确切大小不敏感，从而保证程序的可移植性（portable）。\n寻址和字节顺序 对于多字节的程序对象，我们必须建立两个准则：这个对象的地址是什么和这些字节在内存中的排列顺序是怎样的。某些机器选择按照从最低有效字节到最高有效字节的顺序存储对象，称为小端法（little endian）；而某些则与之相反，称为大端法（big endian）。如一个 int 类型的变量 x 地址为 0x100，其值为十六进制的 0x1234567，那么上述两种机器存储该变量的方式分别如下：\nlittle endian\n    0x100 0x101 0x102 0x103      … 67 45 23 01 …    big endian\n    0x100 0x101 0x102 0x103      … 01 23 45 67 …    有些时候不同的存储顺序可能导致一些问题：\n 不同类型的机器通过网络传递二进制数据，如小端法机器产生的数据发送到大端机器上，得到的字节序列是反的。这就要求发送方的机器需要将代码转换为网络标准，接收方再将其转化为其内部的表达方式； 对于小端法机器，书写字节序列与书写数字的顺序相反； 某些使用强制类型转换（cast）的程序，在不同类型的机器上编译运行的结果不同。  对于上述第三种情况，我们以一个程序为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #include \u003cstdio.h\u003e typedef unsigned char *byte_pointer; void show_bytes(byte_pointer start, size_t len) { size_t i; for (i = 0; i \u003c len; i++) { // %.2x 代表整数会被打印为至少两位（digits）的十六进制数字  printf(\"%.2x\", start[i]); } printf(\"\\n\"); } void show_int(int x) { show_bytes((byte_pointer)\u0026x, sizeof(int)); } void show_float(float x) { show_bytes((byte_pointer)\u0026x, sizeof(float)); } void show_pointer(void *x) { show_bytes((byte_pointer)\u0026x, sizeof(void *)); } int main() { int val = 12345; float fval = (float) val; int *pval = \u0026val; show_int(val); show_float(fval); show_pointer(pval); }   不同类型机器的输出结果如下：\n   Machine val fval pval     Linux32 0x39300000 0x00e44046 0xe4f9ffbf   Windows32 0x39300000 0x00e44046 0xb4cc2200   Sun 0x00003039 0x4640e400 0xeffffa0c   Linux64 0x39300000 0x00e44046 0xb811e5ffff7f0000    这是因为 Sun 系统采用大端法，其他三者采用小端法。对于指针类型的变量，不同操作系统在存储分配上有着不同的准则。同时，64 位系统使用 8 字节地址，32 位系统使用 4 字节地址，这导致了指针类型的变量 pval 输出结果的不同。\n字符串 字符串在 C 中被编码为一个以 null 字符结尾（其值为 0）的字符数组，每个字符都由某种标准编码组成，如 ASCII 码。因此，如果我们执行上面的程序show_bytes(\"12345\", 6)，将得到 31 32 33 34 35 00。\n由于字符串中各字符的排列顺序是由字符串本身决定的，因此字符串不会受到大端法/小端法的影响，除非字符使用 16 位两字节的 Unicode 进行编码。\n代码 不同类型的机器使用不同且不兼容的指令和编码方式，因此二进制代码很少能在不同机器和操作系统组合之间移植。\n布尔代数简介 几种布尔运算符的定义如下：\n ～：非，相当于逻辑运算的 NOT； \u0026：与，相当于逻辑运算的 AND； ｜：或，相当于逻辑运算的 OR； ^：异或，相当于逻辑运算的 EXCLUSIVE-OR。若 $p = 0$，$q = 1$ 或 $p = 1$，$q = 0$ 时，$p \\text{\\textasciicircum} q = 1$。  布尔运算符可以应用于位向量，即固定长度的 0、1 序列。举例来说，若 a 为 [0110]，b 为 [0101]，那么：\n   Operation Result     ~a [1001]   a \u0026 b [0100]   a | b [0111]   a ^ b [0011]    C 中的位级运算 C 中支持按位布尔运算，上面提到的布尔运算符其实就是在 C 中使用的。一个使用布尔运算符的经典程序如下：\n1 2 3 4 5 6 7 8  #include \u003cstdio.h\u003e void inplace_swap(int *x, int *y) { *y = *x ^ *y; *x = *x ^ *y; *y = *x ^ *y; }   利用对于任意数 $a$ ，$a \\text{\\textasciicircum} a$ = 0 以及 $0 \\text{\\textasciicircum} a = a$ 这一性质，该程序不使用中间变量便完成了变量值的交换。另外，上述方法的实现还建立在异或运算满足交换律和结合律的基础之上。\nC 中的逻辑运算 C 还提供了逻辑运算符，即 || 、\u0026\u0026 和 !。它们和位级运算的一个区别就是，如果对第一个参数求值就能确定表达式的结果，那么就不会对第二个参数进行求值。如表达式 a \u0026\u0026 5 / a 不会导致除数为 0 的异常，而 p \u0026\u0026 *p++ 也不会导致简介引用空指针。\nC 中的移位运算 C 中的移位运算有左移和右移两种，均不会改变位向量的长度。左移运算 $x « k$ 就是 x 向左移动 k 位，丢弃 k 个高位，并在右端补充 k 个 0。通常可以使用 $x « 1$ 和 $x « 2$ 分别代替 $x \\times 2$ 和 $x \\times 4$，因为位级运算拥有相比乘法更快的运算速度。\n右移运算分为两种形式，逻辑和算术。无符号数据必须使用逻辑右移，$x » k$ 将 x 的左端补充 k 个 0 ，并丢弃 k 个低位。而大多数机器使用算术右移处理有符号数据，$x » k$ 将 x 的左端补充 k 个最高有效位的拷贝，并丢弃 k 个低位。\n加减乘除运算符的优先级大于移位运算符，因此 $1 « 2 + 3 « 4$ 等效于 $1 « (2 + 3) « 4$。\n整数的表示 无符号编码 一个长度为 $w$ 的位向量 $\\vec{x} = [x_{w-1},x_{w-2},…,x_0]$，它从二进制（Binary）转化为无符号编码（Unsigned）的公式为：\n$$ B2U_w(\\vec{x}) \\doteq \\displaystyle\\sum_{i=0}^{w - 1}x_i2^i $$\n其中， $\\doteq$ 符号表示等式的左手边被定义为右手边。无符号编码的最小值为位向量 [00…0]，即整数值 0。最大值为位向量 [11…1]，即整数值 $U\\max_w \\doteq \\displaystyle\\sum_{i=0}^{w - 1}2^i = 2^w -1$。函数 $B2U_w$ 是一个双射（bijection）：对于每个长度为 $w$ 的位向量，都有唯一的整数值与之对应，反之亦然。\n二进制补码 二进制补码（Two’s-Complement Encoding）将位向量的符号位（即最高有效位）作为负权重（negative weight），符号位为 1 代表值为负，符号位为 0 代表值为正。如一个长度为 $w$ 的位向量 $\\vec{x} = [x_{w-1},x_{w-2},…,x_0]$，它从二进制转化为二进制补码的公式为：\n$$ B2T_w(\\vec{x}) \\doteq -x_{w - 1}2^{w - 1} + \\displaystyle\\sum_{i=0}^{w - 2}x_i2^i $$\n二进制补码的最小值为位向量 [10…0]，即整数值 $T\\min_w \\doteq -2^{w-1}$。最大值为位向量 [01…1]，即整数值 $T\\max_w \\doteq \\displaystyle\\sum_{i=0}^{w - 2}2^i = 2^{w-1} -1$。与二进制转化无符号编码类似，函数 $B2T_w$ 也是一个双射。而它们的最值之间有着如下性质：\n$$ |T\\min| =|T\\max| + 1$$ $$ U\\max = 2T\\max + 1$$\n特别地，整数 -1 和 $U\\max_w$的位级表示均为全 1：[11…1]，而整数 0 在两种表达方式中均为全 0：[00…0]。虽然 C 的标准并没有限制有符号整数的二进制表达，但大多数机器都采用了二进制补码的方式。\n有符号数和无符号数的转换 在 C 中，有符号数和无符号数之间的转换是基于位级视角的，而非数字。例如：\n1 2 3  short int v = -12345; unsigned short uv = (unsigned short) v; printf(\"v = %d, uv = %u\\n\", v, uv);   输出结果为 v = -12345，uv = 53191。这意味着在类型转换过程中位向量不变，但位向量转换到整数值的方式不同。根据推导，二进制补码转换为无符号数的公式如下：\n$$T2U_w(x) = \\begin{cases} x + 2^w \u0026\\text x \u003c 0 \\cr x \u0026\\text x \\geq 0 \\end{cases}$$\n如图所示，非负数转换前后保持不变，负数则变成了一个较大的正数：\n而无符号数转换为二进制补码的公式则为：\n$$U2T_w(u) = \\begin{cases} u \u0026\\text u \\leq T\\max_w \\cr u - 2^w \u0026\\text u \u003e T\\max_w \\end{cases}$$\n如图所示，小于 $2 ^ {w - 1}$ （最高有效位为 0）的数转换前后保持不变，大于等于 $2 ^ {w - 1}$ （最高有效位为 1）的数将被转换为一个负数：\n通过上面的讨论我们发现，大于等于 0 且小于 $2 ^ {w - 1}$ 的值有相同的无符号和二进制补码表示。而这个范围之外的数，在转换过程中需要加上或减去 $2 ^ w$。\nC 中的有符号数和无符号数 通常大多数数字默认都是有符号的，除非加上后缀 U 或 u，如 12345U 和 0x1A2Bu 等。除了显式地使用强制类型转换（如上一节中的程序）以外，也可以将一种类型的表达式赋值给另一种变量，即隐式转换：\n1 2 3 4  int tx, ty; unsigned ux, uy; tx = ux; /* Cast to signed*/ uy = ty; /* Cast to usigned*/   如果参与运算的两个数一个是有符号的，一个是无符号的，那么 C 会隐式地将有符号数转换为无符号数并假定两数均为非负后再进行计算。比如表达式 -1 \u003c 0U 的值为 false，因为 -1 会先转换为无符号数再与 0 进行比较。\n扩展一个数的位级表示 要将一个无符号数转换为更大的数据类型，只需简单地在头部添加 0，这种运算被称为零扩展（zero extension）。而对于二进制补码，则需要在头部添加最高有效位（符号位），称为符号扩展（sign extension）。\n当类型转换既改变数据类型的大小又改变符号类型时，则先改变大小，再改变有无符号。例如将一个 short 类型的变量转换为 unsigned 类型，实际上先将它转换为了 int 类型，再从 int 转换为了 unsigned。\n截断数字 将一个数转换为更小的数据类型时，截断数字的位数是不可避免的。如一个 $w$ 位的位向量 $\\vec{x} = [x_{w-1},x_{w-2},…,x_0]$截断为 $k$ 位时，我们会丢弃 $w-k$ 个高位，得到 ${\\vec{x} = [x_{k-1},x_{k-2},…,x_0]}$。截断数字可能会导致值的变化：\n$$B2U_k([x_{k-1},x_{k-2},…,x_0]) = B2U_w([x_{w-1},x_{w-2},…,x_0]) \\bmod 2^k $$ $$B2T_k([x_{k-1},x_{k-2},…,x_0])= U2T_k(B2U_w([x_{w-1},x_{w-2},…,x_0]) \\bmod 2^k)$$\n通过上面几小节的讨论，我们发现无符号数与有符号数之间的隐式转换导致了一些与常识相悖的运算结果，这将导致一些很难发现的程序错误。因此很多编程语言，如 Java，不支持无符号数的使用。\n整数的运算 无符号加法 两个可用 $w$ 位无符号编码表示的非负整数 $x$ 和 $y$，其范围：$0 \\leq x, y \\leq 2^w -1$，那么它们的和：$0 \\leq x+ y \\leq 2^{w + 1} -2$ 就有可能需要用 $w+1$ 位来表示。如果出现溢出便丢弃高位，因此无符号加法（$+_w^u$）等价于计算 $(x+y) \\bmod 2^w$，即：\n$$x + _w^uy = \\begin{cases} x + y \u0026\\text x + y \u003c 2^w \u0026\\text Normal \\cr x + y - 2^w \u0026\\text 2^w \\leq x + y \u003c 2^{w+1} \u0026\\text \\space \\space \\space Overflow \\end{cases}$$\n模数加法构成了一种数学结构，称为阿贝尔群，它是可交换的和可结合的。$w$ 位无符号数的集合执行 $+_w^u$ 运算，对于每个值 $x$，必然有某个值 $-_w^ux$ 满足 $-_w^ux +_w^ux = 0$，该值称为 $x$ 的逆元。当 $x = 0$ 时，其逆元自然为 0。当 $x \u003e 0$ 时，显然 $ (x + 2^w - x)\\bmod 2^w = 0$。由于 $0 \u003c 2^w -x \u003c 2^w$，因此 $2^w -x$ 便是 $x$ 的逆元。上述两种情况总结如下：\n$$ -_w^ux = \\begin{cases} x \u0026\\text x = 0 \\cr 2^w -x \u0026\\text x \u003e 0 \\end{cases}$$\n二进制补码加法 两个数的 $w$ 位二进制补码之和（$+_w^t$）与无符号之和有着完全相同的位级表示，因此对于 $-2^{w-1} \\leq x, y \\leq 2^{w-1} -1$ ，有：\n$$\\begin{split} x +_w^ty \u0026=U2T_w(T2U_w(x) +_w^uT2U_w(y))\\cr \u0026=U2T_w[(x+y)\\bmod 2^w] \\end{split}$$\n进一步地，我们根据两数之和的范围，将上述结果分情况讨论，从而得到：\n$$x + _w^ty = \\begin{cases} x + y + 2^w \u0026\\text x + y \u003c -2^{w-1} \u0026\\text \\space \\space \\space Negative \\space \\space Overflow \\cr x + y \u0026\\text -2^{w-1} \\leq x + y \u003c 2^{w-1} \u0026\\text Normal \\cr x + y - 2^w \u0026\\text x + y \\geq 2^{w-1} \u0026\\text \\space \\space \\space Postive \\space \\space Overflow \\end{cases}$$\n因此，若 $x\u003e0, y\u003e0, x+_w^ty\\leq 0$，那么结果便出现了正溢出；若 $x\u003c0, y\u003c0, x+_w^ty\\geq 0$，结果便出现了负溢出。\n二进制补码的逆元计算公式如下：\n$$ -_w^tx = \\begin{cases} T\\min_w \u0026\\text x = T\\min_w \\cr -x \u0026\\text x \u003e T\\min_w \\end{cases}$$\n无符号乘法 无符号乘法运算（$\\times_w^u$）与加法类似，都可以转换为对 $2^w$ 的模运算：\n$$x\\times_w^uy=(x \\times y)\\bmod2^w$$\n二进制补码乘法 同样与加法类似，两个数的 $w$ 位二进制补码之积（$\\times_w^t$）与无符号之积有着完全相同的位级表示，因此：\n$$\\begin{split} x \\times_w^ty \u0026=U2T_w(T2U_w(x)\\times_w^uT2U_w(y))\\cr \u0026=U2T_w[(x \\times y)\\bmod 2^w] \\end{split}$$\n乘以常数 整数乘法运算在许多机器上运行缓慢，一个重要的优化便是使用移位运算和加法运算来代替它。我们首先考虑乘以 2 的幂的情况，然后推广到任意常数。若存在位向量 $[x_{w-1},x_{w-2},…,x_0]$，$x \\times 2^k$ 可以表示为在其右端添加 $k$ 个 0，即 $[x_{w-1},x_{w-2},…,x_0,0,…,0]$。因此在 C 中，对于整数 $x$ （无论是无符号数还是二进制补码）和 无符号数 $k$，$x \\times _w^t2^k$ 就等于 $x«k$。\n如果一个常数可以拆分为 2 的幂的和，那么我们便可以使用左移运算和加（减）法运算来替换与相关的乘法运算。如 $14=2^3+2^2+2^1$，那么 $x\\times14=(x«3)+(x«2)+(x«1)$。\n除以 2 的幂 我们使用右移运算来代替除法运算，而逻辑右移和算术右移分别适用于无符号数和二进制补码。由于结果为整数，因此很可能需要进行舍入（round）。我们定义 $⌊ ⌋$ 为向下取整，$⌈ ⌉$ 为向上取整。如 $⌊3.14⌋=3$，$⌊-3.14⌋=-4$，而 $⌈3.14⌉=4$，$⌈-3.14⌉=-3$。\n在 C 中，对于无符号数 $x, k$， $x»k=⌊x/2^k⌋$；对于二进制补码 $x$ 和无符号数 $k$，则有 $x»k=⌊x/2^k⌋$。前者为逻辑右移，后者为算术右移。\n考虑到若 $x\u003c0$，$x/y$ 的结果应为 $⌈x/y⌉$，而非 $⌊x/y⌋$。我们可以利用性质：$⌈x/y⌉=⌊(x+y-1)/y⌋$，来修正这种不合适的舍入。因此，对于 $x\u003c0$ 的二进制补码除法，应使用：$(x+(1«k)-1)»k=⌈x/2^k⌉$。综上，二进制补码除以 2 的幂 $x/2^k$ 可以用三元运算符表示为：\n1  (x\u003c0 ? x+(1\u003c\u003ck)-1 : x) \u003e\u003e k   与乘法不同，除法无法推广到任意常数。\n浮点 二进制小数 十进制小数的表示方法为 $d_md_{m-1}…d_1d_0.d_{-1}d_{-2}…d_{-n}$，其中 $d_i$ 为 0～9 的整数。那么该数的大小为：\n$$d=\\displaystyle\\sum_{i=-n}^{m}10^id_i$$\n小数点左边的数的权值为 10 的正幂，右边的则为 10 的负幂。类似地，我们可以得出二进制小数的表示方法。$b_i$ 为 0 或 1，则二进制数 $b_mb_{m-1}…b_1b_0.b_{-1}b_{-2}…b{-n}$ 的值为：\n$$b=\\displaystyle\\sum_{i=-n}^{m}2^ib_i$$\n二进制小数点向左移动一位，相当于数字除以 2。向右移动一位，则相当于数字乘以 2。这种方法只能表示可转化为 $x \\times 2^y$ 形式的数，无法精确表示如 $\\frac{1}{3}$、$\\frac{5}{7}$ 这样的数。\nIEEE 浮点数表示 二进制小数的表示方法难以表示很大的数，我们更希望通过给定 $x, y$ 的值来表示形如 $x \\times 2^y$ 的数。IEEE 浮点数标准使用 $V=(-1)^s\\times M \\times 2^E$ 的形式来表示小数：\n 符号 $s$：为 1 代表负值，为 0 代表正值； 有效数 $M$：一个二进制小数，范围在 1 到 $2-\\varepsilon$ 或 0 到 $1-\\varepsilon$ 之间； 指数 $E$：2 的幂指数，有可能是负数。  因此，浮点数的位级表达分为了三个部分：\n 一个符号位 $s$; $k$ 位的指数域 $exp=e_{k-1}…e_1e_0$ 编码指数 $E$； $n$ 位的小数域 $frac=f_{n-1}…f_1f_0$ 编码有效数 $M$。  C 中的 float 类型，$s=1, k=8, n=23$。而对于 double 类型，$s=1, k=11, n=52$。IEEE 浮点数表示法有三种情况，如下图所示：\n第一种情况是最常见的，即指数域不全为 0，也不全为 1。在这种情况下，指数域的值为 $E=e-Bias$，其中 $e$ 是一个位级表达为 $e_{k-1}…e_1e_0$ 的无符号数，$Bias$ 则是一个 $2^{k-1}-1$ 的常数。小数域的值为 $M=1+f$，其中，$f$ 是一个二进制小数 $0.f_{n-1}…f_1f_0$。\n第二张情况是指数域全为 0，这样所表示的数就是非标准化形式的。在这种情况下，$E=1-Bias, M=f$。非标准化数可以表示第一种情况无法表示的 0 以及非常接近 0 的数字。\n第三种情况是指数域全为 1 时出现的。若小数域全为 0，得到的值则为 $\\pm \\infin$。若小数域不全为 0，则结果为 NaN，即不是一个数字。比如计算 $\\infin -\\infin$ 和 $\\sqrt{-1}$，就会得到这样的结果。\n以 8 位浮点数为例，$s=1, k=4, n=3$，此时偏移量 $Bias=2^{4-1}-1=7$。最靠近 0 的是非标准化数，$E=1-Bias=-6$，$2^E=\\frac{1}{64}$，$M=f=0,\\frac{1}{8},…,\\frac{7}{8}$，因此浮点数 $V$ 的范围就是 0 ～$\\frac{7}{8\\times64}=\\frac{7}{512}$。而对于最小的标准数来说，指数域为 [0001]，因此 $E=e-Bias=-6$，小数域 $M=1+f=1,\\frac{9}{8},…\\frac{15}{8}$，浮点数 $V$ 的范围为 $\\frac{8}{512}=\\frac{1}{64}$ ~ $\\frac{15}{512}$。\n我们可以观察到最大非标准数和最小标准数分别为 $\\frac{7}{512}$ 和 $\\frac{8}{512}$，这种平滑的过渡得益于我们将非标准数的 $E$ 使用 $1-Bias$ 来计算，而非 $-Bias$。\n在这种条件下，当指数域为 [1110]，$E=e-Bias=7, 2^E=128$，小数域 $M=1+0.111_2=\\frac{15}{8}$ 时，$V$ 取到最大值 240，超出这个值就会溢出到 $+\\infin$。值得一提的是，IEEE 浮点数可以使用整数排序函数来进行排序。\n舍入 对浮点数的表示限制了其范围和精度，因此浮点计算只能近似地表示实数计算。IEEE 浮点数格式定义了四种不同的舍入方式：\n向偶数舍入（Round-to-even），也称向最接近的数舍入（Round-to-nearest），是默认的方法。它试图找到一个最接近的匹配值，对于中间值（如表中的 1.5），则使结果的最低有效位为偶数（舍入为 2）。其他三种方法用来确定值的上下界。\n即使在舍入到小数的情况下，也可以使用整数舍入，只需简单地考虑最低有效数字是偶数还是奇数。如保留两位小数，我们把十进制小数 1.234999 舍入到 1.23，把 1.235001 舍入到 1.24，而 1.235 和 1.245 均舍入到 1.24。这种方法同样可以推广到二进制小数，此时应将中间值舍入到最低有效位等于 0 的数。\n浮点运算 浮点数的加法和乘法是实际运算后进行舍入后的结果，即对于实数 $x, y$，以及运算 $\\odot$，结果为 $Round(x\\odot y)$。IEEE 标准规定了浮点数运算的行为，这意味着它不依赖于任何具体的硬件或软件，从而实现了可移植性。\n上文提到整数的加法和乘法形成了阿贝尔群，而实数亦如此，但浮点数还要考虑舍入对其特性的影响。浮点数 $\\infin$ 和 NaN 没有逆元，加法也只满足交换律但不满足结合律。类似地，浮点数乘法只满足交换律，而不满足结合律和分配律。这几种特性的缺乏，对程序员有着非常重要的影响。例如下面的简单程序：\n1 2  x = a + b + c; y = b + c + d;   编译器可能试图产生如下代码来省去一个浮点加法从而提升运算效率：\n1 2 3  t = b + c; x = t + a; y = t + d;   由于使用了加法结合律，计算的结果就有可能产生不同。因此大多数编译器倾向于非常保守，避免任何可能对功能造成影响的优化。\n","description":"","tags":["CSAPP","OS"],"title":"CSAPP 读书笔记：信息的表示和处理","uri":"/posts/representing-and-manipulating-information-note/"},{"categories":null,"content":"信息就是 Bits + Context 一堆 bit 可以表示系统中的所有信息，包括磁盘中的文件、内存中的程序和用户数据以及网络中传输的数据，区分它们的唯一方式便是我们查看这些数据对象时所处的上下文（Context）。例如，相同的一串 bit 在不同的 Context 中可能代表一个整数，也可能代表一个浮点数，甚至字符串。\n程序的转化过程 一个简单的 C 程序 hello.c 如下：\n1 2 3 4 5 6 7  #include \u003cstdio.h\u003e int main() { printf(\"hello world\\n\"); return 0; }   高级的 C 程序文件 hello.c 被转化为一系列低级的机器语言指令，最后以二进制可执行文件存储在磁盘中。\n 预处理阶段（Preprocessor）：预处理器修改 C 程序文件中以#号开头的命令。如 hello.c 中的#include \u003cstdio.h\u003e命令将会告诉预处理器系统头文件 stdiio.h 的内容，然后将其直接插入到程序文本中。生成的新程序文件为 hello.i； 编译阶段（Compilation）：编译器将 hello.i 文件转化为由汇编语言组成的 hello.s 文件。每条汇编语句都描述了一条低级的机器语言指令，不同高级语言编译后的汇编语句是通用的； 汇编阶段（Assembly）：汇编器将 hello.s 文件转化为由二进制机器语言指令的 hello.o 文件。如果我们用文本编辑器打开该文件，将会展现出一堆乱码； 链接阶段（Linking）：由于我们的程序调用了printf函数，而它存在于一个名为 printf.o 的预编译文件中。链接器负责将该文件并入，得到最终的可执行文件 hello。  系统的硬件组成  总线（Buses）：贯穿整个系统的一组电子管道，负责在各个组件之间传递给定大小的字节块（称为word）。word的大小是系统的基本参数，一般有 4 字节（32 位）或 8 字节（64 位）两种； I/O 设备：系统与外部世界连接的桥梁。图中的 I/O 设备有用于用户输入的键盘⌨️和鼠标🖱️、用于展示的用户输出以及用于长期存储数据和程序的磁盘驱动，每个 I/O 设备都通过控制器（Controller）或适配器（Adapter）与 I/O 总线相连。其中，控制器是设备自身或系统主板（Motherboard）上的芯片组，而适配器则是插在主板插槽上的卡； 主存储器（Main Memory）：处理器执行程序时存放程序和数据的临时存储。物理上来说，内存是由动态随机存取存储器（DRAM, Dynamic Random Access Memory）芯片组成的集合。而逻辑上则是一个线性的字节数组，每个字节都有其唯一地址（从 0 开始的数组索引）； CPU（Central Processing Unit ）: 解释或执行主存储器中指令的引擎。  PC：CPU 的核心是一个大小与word相同的存储设备（或寄存器），称为程序计数器（Program Counter）。PC 始终指向主存储器中某条机器语言指令，即内含其地址。CPU 会不断地重复执行 PC 指向的机器指令，并更新 PC 使其指向下一条指令； Register file：寄存器文件是一个小型存储设备，由一组word大小的寄存器组成，而每个寄存器都有自己的唯一名称； ALU： 算术/逻辑单元（Arithmetic/Logic Unit），能够计算新的数据和地址值。    程序的运行过程 从键盘上读取命令：当我们在终端中输入命令./hello后，bash 程序将逐一读取命令字符串到寄存器（Register），然后存储于内存中；\n从磁盘加载可执行文件到内存：当我们输入回车键后，bash 程序得知输入结束，于是开始加载可执行文件 hello，其中的代码和数据将通过直接存储器访问技术（DMA, Direct Memory Access）从磁盘拷贝到内存中；\n从内存中将结果输出到显示器：处理器执行 hello 文件中的机器语言指令，然后将hello world\\n字符串中的字节从内存拷贝到寄存器文件中，最终传输到用于展示的屏幕🖥上。\n高速缓冲存储 在程序运行的过程中，操作系统花费了大量时间将信息从一个地方拷贝另一个地方，CPU 从寄存器文件中读取数据要比从主存储器中读取快近百倍。因此系统设计者引入了一种更小、更快的存储设备，称为高速缓冲存储（Cache Memories or Caches），它会暂存 CPU 在短期内需要用到的数据。\nL1 级别的 Caches 位于 CPU 芯片之上，容量为上万字节并且拥有和寄存器文件相当的访问速度。而 L2 级别的 Caches 则通过一条特殊总线连接到 CPU，容量可达十万到百万字节。虽然其访问速度比 L1 Cache 慢五倍左右，但依然比主存储器要快五到十倍。在某些先进的操作系统中，还会使用 L3 级别的 Cache，它们均是通过静态随机存取存储器（SRAM, Static Random Access Memory）实现的。\n计算机系统中的存储器层级结构如下图所示，低层次的存储器作为相邻高层次存储器的 Cache：\n操作系统对硬件的管理 操作系统是应用程序和硬件的中间层，应用程序对硬件的所有操作必须通过操作系统实现。\n操作系统有两个主要功能：防止硬件被失控的应用程序所滥用；为应用程序提供一种简单而统一的机制来处理复杂且通常差异很大的低级硬件设备。上述两种功能是通过下图中的几个基本抽象实现的：\n文件（FIles）是对 I/O 设备的抽象，虚拟内存 (Virtual Memory) 是对主存储器和 I/O 设备的抽象，而进程（Processes）则是对处理器，主存储器和 I/O 设备的抽象。\n进程 进程是操作系统对正在运行的程序的抽象，它让我们的 hello 程序看起来像是系统中唯一运行的程序。多个进程可以并行地在同一个系统中运行，同时每个进程都好像在独占硬件的使用权。而实际上不同进程中的指令是交错执行的，基于下图中的上下文切换：\n操作系统会跟踪进程运行所需的所有状态信息，即上下文（Context），它包括了程序计数器 PC 的当前值，寄存器文件和主存储器的内容之类的信息。 在任何时间点，单处理器系统只能为单个进程执行代码。 当操作系统决定将控制权从当前进程转移到某个新进程时，它需要首先保存当前进程的上下文，然后还原新进程的上下文，最后将控制权传递给新进程以完成上下文切换。\n进程间的转换是由操作系统内核（kernel）管理的。kernel 并不是一个单独的进程，而是操作系统代码的一部分，始终存在于内存中。当一个应用程序需要操作系统完成一些操作，比如读写文件时，它便会执行一个特殊的系统调用指令，然后将控制权移交给 kernel。kernel 负责实现程序需要进行的操作，并将结果返回给程序。\n线程 每个进程可以由多个执行单元（线程）组成。由于每个线程都运行在进程的上下文中，且不同线程之间可以共享进程内的代码和全局数据，因此线程要比进程更加高效。\n虚拟内存 虚拟内存让每个进程都看起来独占了主存储器的使用权。每个进程看到的内存空间都是相同的，称为虚拟地址空间（VIrtual Address Space），其组成如下：\n 程序代码和数据：所有进程的代码都始于相同的固定地址，随后则是与全局变量相关的数据区。它们的大小在进程开始运行时固定； 堆（Heap）：运行时堆是调用malloc或free这样的 C 标准库生成的结果，其大小可以在进程运行时动态扩缩容； 共享库（Share libraries）：存放如 C 标准库、数学库这样的共享库的代码和数据的区域； 栈（Stack）：编译器实现函数调用的区域，其大小同样可以在进程运行时动态扩缩容。如果我们调用一个函数，栈就会增长。而每当一个函数返回时，栈便会缩小； 内核虚拟内存：为 kernel 预留的内存空间。  文件 文件是由字节组成的序列，因此所有的 I/O 设备（包括网络）都可以被看作文件。系统中的所有输入和输出都可以通过 Unix I/O（一组系统调用），对文件进行读写来实现。\n多处理器系统 由单一操作系统内核控制的多个处理器可以共同组成一个多处理器系统（Multiprocessor System），它基于多核（Multi-core）处理器以及超线程技术（Hyperthreading）。\n多核处理器 多核处理器将多个 CPU 集成到单个集成电路芯片中，每个 CPU 称为一个核（cores）。下图展示了一个典型的多核处理器的架构。其中 L1 级别的 Cache 被分成了两部分，分别存储短期内需要使用的指令（i-cache）和数据（d-cache）：\n超线程技术 超线程技术允许单个 CPU 执行多个控制流，有时也被称为同步多线程（Simultaneous Multi-threading）。通过对 CPU 中的程序计数器、寄存器文件等硬件资源进行拷贝，将一个物理 CPU 虚拟为多个逻辑 CPU，从而实现多个线程的并行计算。常规的 CPU 需要大约两万个时钟周期（clock cycle）完成不同线程间的切换，而超线程的 CPU 可以在单个时钟周期内决定要执行哪一个线程，这使得 CPU 能够更好地利用它的执行资源。比如一个逻辑 CPU 执行的线程需要等待数据加载到 Cache 中，那么另一个逻辑 CPU 就可以向其借用执行资源继续执行其他线程。\n与使用多个物理 CPU 的传统多处理器系统不同，超线程内核中的逻辑 CPU 共享执行资源。因此当两个线程同时需要某个执行资源时，其中一个线程必须让出资源暂时挂起，直到这些资源空闲后才能继续执行。\n","description":"","tags":["CSAPP","OS"],"title":"CSAPP 读书笔记：计算机系统之旅","uri":"/posts/a-tour-of-computer-systems-note/"},{"categories":null,"content":"前言 在 Kubernetes Pod 是如何跨节点通信的？ 中，我们简单地介绍了 Kubernetes 中的两种 SDN 网络模型：Underlay 和 Overlay。而 Openshift 中的 SDN 则是由 Overlay 网络 OVS（Open vSwitch）实现的，其使用的插件如下：\n ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信； ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 oc get netnamespaces 命令查看； ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。  OVS 在每个 Openshift 节点上都创建了如下网络接口：\n br0：OpenShift 创建和管理的 OVS 网桥，它会使用 OpenFlow 流表来实现数据包的转发和隔离； vxlan0：VxLAN 隧道端点，即 VTEP（Virtual Tunnel End Point），用于集群内部 Pod 之间的通信； tun0：节点上所有 Pod 的默认网关，用于 Pod 与集群外部和 Pod 与 Service 之间的通信； veth：Pod 通过veth-pair连接到br0网桥的端点。  使用 ovs-ofctl -O OpenFlow13 show br0 命令可以查看br0上的所有端口及其编号：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  [root@node1 ~]# ovs-ofctl -O OpenFlow13 show br0 OFPT_FEATURES_REPLY (OF1.3) (xid=0x2): dpid:0000ea00372f1940 n_tables:254, n_buffers:0 capabilities: FLOW_STATS TABLE_STATS PORT_STATS GROUP_STATS QUEUE_STATS OFPST_PORT_DESC reply (OF1.3) (xid=0x3): 1(vxlan0): addr:72:23:a0:a9:14:a7 config: 0 state: 0 speed: 0 Mbps now, 0 Mbps max 2(tun0): addr:62:80:67:c6:38:58 config: 0 state: 0 speed: 0 Mbps now, 0 Mbps max 8381(vethd040c191): addr:7a:d9:f4:12:94:5f config: 0 state: 0 current: 10GB-FD COPPER speed: 10000 Mbps now, 0 Mbps max ... LOCAL(br0): addr:76:ab:cf:6f:e1:46 config: PORT_DOWN state: LINK_DOWN speed: 0 Mbps now, 0 Mbps max OFPT_GET_CONFIG_REPLY (OF1.3) (xid=0x5): frags=nx-match miss_send_len=0   考虑到 Openshift 集群的复杂性，我们分别按以下几种场景分析数据包的流向：\n 节点内 Pod 互访：Pod to Local Pod Pod 跨节点互访：Pod to Remote Pod Pod 访问 Service：Pod to Service Pod 与集群外部互访：Pod to External  由于高版本（3.11 以上）的 Openshift 不再以守护进程而是以 Pod 的形式部署 OVS 组件，不方便对 OpenFlow 流表进行查看，因此本文选用的集群版本为 3.6：\n1 2 3 4 5 6 7 8  [root@node1 ~]# oc version  oc v3.6.173.0.5 kubernetes v1.6.1+5115d708d7 features: Basic-Auth GSSAPI Kerberos SPNEGO Server https://test-cluster.ocp.koktlzz.com:8443 openshift v3.6.173.0.5 kubernetes v1.6.1+5115d708d7   另外，实验用集群并未开启 ovs-multitenant，即未进行多租户隔离。整个集群 Pod 网络是扁平化的，所有 Pod 的 VNID 都为默认值 0。\nPod to Local Pod 数据包首先通过veth-pair送往 OVS 网桥br0，随后便进入了br0上的 OpenFlow 流表。我们可以用 ovs-ofctl -O OpenFlow13 dump-flows br0 命令查看流表中的规则，同时为了让输出结果更加简洁，略去 cookie 和 duration 的信息：\n table=0, n_packets=62751550874, n_bytes=25344802160312, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.130.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]-\u003eNXM_NX_REG0[],goto_table:10 table=0, n_packets=1081527047094, n_bytes=296066911370148, priority=200,ip,in_port=2 actions=goto_table:30 table=0, n_packets=833353346930, n_bytes=329854403266173, priority=100,ip actions=goto_table:20  table0 中关于 IP 数据包的规则主要有三条，其中前两条分别对应流入端口in_port为 1 号端口vxlan0和 2 号端口tun0的数据包。这两条规则的优先级priority都是 200，因此只有在两者均不符合情况下，才会匹配第三条规则。由于本地 Pod 发出的数据包是由veth端口进入的，因此将转到 table20；\n table=20, n_packets=607178746, n_bytes=218036511085, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0-\u003eNXM_NX_REG0[],goto_table:21 table=21, n_packets=833757781068, n_bytes=329871389393381, priority=0 actions=goto_table:30  table20 会匹配源地址nw_src为 10.130.9.154 且流入端口in_port为 8422 的数据包，随后将 Pod1 的 VNID 0 作为源 VNID 存入寄存器 0 中，经由 table21 转到 table30；\n table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70 table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90 table=30, n_packets=21061319859, n_bytes=29568807363654, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60 table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100  table30 中匹配数据包目的地址nw_dst的规则有四条，前三条分别对应本节点内 Pod 的 CIDR 网段 10.130.8.0/23、集群内 Pod 的 CIDR 网段 10.128.0.0/14 和 Service 的 ClusterIP 网段 172.30.0.0/16。第四条优先级最低，用于 Pod 对集群外部的访问。由于数据包的目的地址 10.130.9.158 符合第一条规则，且第一条规则的优先级最高，因此将转到 table70；\n table=70, n_packets=597219981, n_bytes=243824445346, priority=100,ip,nw_dst=10.130.9.158 actions=load:0-\u003eNXM_NX_REG1[],load:0x20ea-\u003eNXM_NX_REG2[],goto_table:80  table70 匹配目的地址nw_dst为 Pod2 IP 10.130.9.158 的数据包，并将 Pod2 的 VNID 0 作为目的 VNID 存入寄存器 1 中。同时端口号0x20ea被保存到寄存器 2 中，然后转到 table80；\n table=80, n_packets=1112713040332, n_bytes=293801616636499, priority=200 actions=output:NXM_NX_REG2[]  table80 比较寄存器 0 和寄存器 1 中保存的源/目的 VNID。若二者一致，则根据寄存器 2 中保存的端口号将数据包送出。\n端口号0x20ea是一个十六进制数字，即十进制数 8426。而 Pod2 正是通过 8426 号端口设备vethba48c6de连接到br0上，因此数据包便最终通过它流入到了 Pod2 中。\n1 2  [root@node1 ~]# ovs-ofctl -O OpenFlow13 show br0 | grep 8426 8426(vethba48c6de): addr:e6:b2:7e:42:41:91   Pod to Remote Pod Packet in Local Pod 数据包依然首先通过veth-pair送往 OVS 网桥br0，随后便进入了br0上的 OpenFlow 流表：\n table=0, n_packets=830232155588, n_bytes=328613498734351, priority=100,ip actions=goto_table:20 table=20, n_packets=1901, n_bytes=299279, priority=100,ip,in_port=6635,nw_src=10.130.9.154 actions=load:0-\u003eNXM_NX_REG0[],goto_table:21 table=21, n_packets=834180030914, n_bytes=330064497351030, priority=0 actions=goto_table:30  与 Pod to Local Pod 的流程一致，数据包根据规则转到 table30；\n table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90 table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70  数据包的目的地址为 Pod2 IP 10.131.8.206，不属于本节点 Pod 的 CIDR 网段 10.130.8.0/23，而属于集群 Pod 的 CIDR 网段 10.128.0.0/14，因此转到 table90；\n table=90, n_packets=15802525677, n_bytes=6091612778189, priority=100,ip,nw_dst=10.131.8.0/23 actions=move:NXM_NX_REG0[]-\u003eNXM_NX_TUN_ID[0..31],set_field:10.122.28.8-\u003etun_dst,output:1  table90 根据目的 IP 的所属网段 10.131.8.0/23 判断其位于 Node2 上，于是将 Node2 IP 10.122.28.8 设置为tun_dst。并且从寄存器 0 中取出 VNID 的值，从 1 号端口vxlan0输出。\nvxlan0作为一个 VTEP 设备（参见 Overlay Network），将根据 table90 发来的信息，对数据包进行一层封装：\n 目的地址（dst IP） –\u003e tun_dst –\u003e 10.122.28.8 源地址（src IP） –\u003e Node1 IP –\u003e 10.122.28.7 源 VNID –\u003e NXM_NX_TUN_ID[0..31] –\u003e 0  由于封装后的数据包源/目的地址均为节点 IP，因此从 Node1 的网卡流出后，可以通过物理网络设备转发到 Node2 上。\nPacket in Remote Pod Node2 上的vxlan0对数据包进行解封，随后从br0上的 1 号端口进入 OpenFlow 流表中：\n table=0, n_packets=52141153195, n_bytes=17269645342781, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.131.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]-\u003eNXM_NX_REG0[],goto_table:10  table0 判断数据包的流入端口in_port、源 IP 所属网段nw_src和目的 IP 所属网段nw_dst均符合该条规则，于是保存数据包中的源 VNID 到寄存器 0 后转到 table10；\n table=10, n_packets=10147760036, n_bytes=4060517391502, priority=100,tun_src=10.122.28.7 actions=goto_table:30  table10 确认 VxLAN 隧道的源 IPtun_src就是节点 Node1 的 IP 地址，于是转到 table30；\n table=30, n_packets=678759566065, n_bytes=172831151192704, priority=200,ip,nw_dst=10.131.8.0/23 actions=goto_table:70  table30 确认数据包的目的 IP（即 Pod2 IP）存在于 Node2 中 Pod 的 CIDR 网段内，因此转到 table70；\n table=70, n_packets=193211683, n_bytes=27881218388, priority=100,ip,nw_dst=10.131.8.206 actions=load:0-\u003eNXM_NX_REG1[],load:0x220-\u003eNXM_NX_REG2[],goto_table:80  table70 发现数据包的目的 IP 与 Pod2 IP 相符，于是将 Pod2 的 VNID 作为目的 VNID 存于寄存器 1 中，将0x220（十进制数 544）保存在寄存器 2 中，然后转到 table80；\n table=80, n_packets=676813794014, n_bytes=172576112594488, priority=200 actions=output:NXM_NX_REG2[]  table80 会检查保存在寄存器 0 和寄存器 1 中的源/目的 VNID，若相等（此例中均为 0），则从 544 号端口输出。\nbr0上的 544 端口对应的网络接口是vethe9f523a9，因此数据包便最终通过它流入到了 Pod2 中。\n1 2  [root@node2 ~]# ovs-ofctl -O OpenFlow13 show br0 | grep 544 544(vethe9f523a9): addr:b2:a1:61:00:dc:3b   Pod to Service 在本例中，Pod1 通过 Service 访问其后端的 Pod2，其 ClusterIP 为 172.30.107.57，监听的端口为 8080：\n1 2 3  [root@node1 ~]# oc get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE myService 172.30.107.57 \u003cnone\u003e 8080/TCP 2y    table=30, n_packets=21065939280, n_bytes=29573447694924, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60  数据包在送到 OpenFlow 流表 table30 前的步骤与 Pod to Local Pod 和 Pod to Remote Pod 中的情况一致，但数据包的目的地址变为了 myService 的 ClusterIP。因此将匹配nw_dst中的 172.30.0.0/16 网段，转到 table60；\n table=60, n_packets=0, n_bytes=0, priority=100,tcp,nw_dst=172.30.107.57,tp_dst=8080 actions=load:0-\u003eNXM_NX_REG1[],load:0x2-\u003eNXM_NX_REG2[],goto_table:80  table60 匹配目的地址nw_dst为 172.30.107.57 且目的端口为 8080 的数据包，并将 Pod1 的 VNID 0 保存到寄存器 1 中，将0x2（十进制数字 2）保存到寄存器 2 中，转到 table80；\n table=80, n_packets=1113435014018, n_bytes=294106102133061, priority=200 actions=output:NXM_NX_REG2[]  table80 首先检查目的 Service 的 VNID 是否与寄存器 1 中的 VNID 一致，然后根据寄存器 2 中的数字将数据包从 2 号端口tun0送出，最后进入节点的 iptables 规则中。\niptables 对数据包的处理流程如下图所示：\n由于 Service 的实现依赖于 NAT（上图中的紫色方框），因此我们可以在 NAT 表中查看到与之相关的规则：\n1 2 3 4 5 6 7 8  [root@node1 ~]# iptables -t nat -nvL Chain OUTPUT (policy ACCEPT 4753 packets, 489K bytes) pkts bytes target prot opt in out source destination 2702M 274G KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ Chain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 4 240 KUBE-SVC-QYWOVDCBPMWAGC37 tcp -- * * 0.0.0.0/0 172.30.107.57 /* demo/myService:8080-8080 cluster IP */ tcp dpt:8080   本机产生的数据包（Locally-generated Packet）首先进入OUTPUT链，然后匹配到自定义链KUBE-SERVICES。由于其目的地址为 Service 的 ClusterIP 172.30.107.57，因此将再次跳转到对应的KUBE-SVC-QYWOVDCBPMWAGC37链：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  Chain KUBE-SVC-QYWOVDCBPMWAGC37 (1 references) pkts bytes target prot opt in out source destination 1 60 KUBE-SEP-AF5DIL6JV3XLLV6G all -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ statistic mode random probability 0.50000000000 1 60 KUBE-SEP-ADAJHSV7RYS5DUBX all -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ Chain KUBE-SEP-ADAJHSV7RYS5DUBX (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 10.131.8.206 0.0.0.0/0 /* demo/myService:8080-8080 */ 0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ tcp to:10.131.8.206:8080 Chain KUBE-SEP-AF5DIL6JV3XLLV6G (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 10.128.10.57 0.0.0.0/0 /* demo/myService:8080-8080 */ 23 1380 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ tcp to:10.128.10.57:8080   KUBE-SVC-QYWOVDCBPMWAGC37链下有两条完全相同的匹配规则，对应了该 Service 后端的两个 Pod。KUBE-SEP-ADAJHSV7RYS5DUBX链和 KUBE-SEP-AF5DIL6JV3XLLV6G链能够执行 DNAT 操作，分别将数据包的目的地址转化为 Pod IP 10.131.8.206 和 10.128.10.57。在一次通信中只会有一条链生效，这体现了 Service 的负载均衡能力。\n完成OUTPUTDNAT 的数据包将进入节点的路由判断（Routing Decision）。由于当前目的地址已经属于集群内 Pod 的 CIDR 网段 10.128.0.0/14，因此将再次从tun0端口再次进入 OVS 网桥br0中。\n1 2 3 4 5 6 7 8 9  [rootnode1 ~]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.122.28.1 0.0.0.0 UG 0 0 0 eth0 10.122.28.0 0.0.0.0 255.255.255.128 U 0 0 0 eth0 10.128.0.0 0.0.0.0 255.252.0.0 U 0 0 0 tun0 169.254.0.0 0.0.0.0 255.255.0.0 U 1008 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 172.30.0.0 0.0.0.0 255.255.0.0 U 0 0 0 tun0   不过数据包在进入br0之前，还需要经过 iptables 中的POSTROUTING链，完成一次 MASQUERADE 操作：数据包的源地址转换为其流出端口的 IP，即tun0的 IP 10.130.8.1。\n1 2 3 4 5 6 7 8 9 10 11  [root@node1 ~]# iptables -t nat -nvL  Chain POSTROUTING (policy ACCEPT 5083 packets, 524K bytes) pkts bytes target prot opt in out source destination 2925M 288G OPENSHIFT-MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* rules for masquerading OpenShift traffic */ Chain OPENSHIFT-MASQUERADE (1 references) pkts bytes target prot opt in out source destination 321M 19G MASQUERADE all -- * * 10.128.0.0/14 0.0.0.0/0 /* masquerade pod-to-service and pod-to-external traffic */ [root@node1 ~]# ip a | grep tun0 16: tun0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN qlen 1000 inet 10.130.8.1/23 scope global tun0   本例中 Service 的后端 Pod 均在 Pod1 所在的节点外，因此数据包第二次进入 OpenFlow 流表时匹配的规则基本与 Pod to Remote Pod 一致：\n table=0, n_packets=1081527047094, n_bytes=296066911370148, priority=200,ip,in_port=2 actions=goto_table:30 table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90 table=90, n_packets=15802525677, n_bytes=6091612778189, priority=100,ip,nw_dst=10.131.8.0/23 actions=move:NXM_NX_REG0[]-\u003eNXM_NX_TUN_ID[0..31],set_field:10.122.28.8-\u003etun_dst,output:1  其传递流程如下图所示：\nPod2 返回的数据包在到达 Node1 后将被vxlan0解封装，然后根据其目的地址tun0进入 OpenFlow 流表：\n table=0, n_packets=1084362760247, n_bytes=297224518823222, priority=200,ip,in_port=2 actions=goto_table:30 table=30, n_packets=20784385211, n_bytes=4742514750371, priority=300,ip,nw_dst=10.130.8.1 actions=output:2  数据包从 2 号端口tun0流出后进入节点的 iptables 规则，随后将触发 iptables 的 Connection Tracking 操作：根据 /proc/net/nf_conntrack 文件中的记录进行“DeNAT”。返回数据包的源/目的地址从 Pod2 IP 10.131.8.206 和 tun0 IP 10.130.8.1，变回 Service 的 ClusterIP 172.30.107.57 和 Pod1 IP 10.130.9.154。\n1 2  [root@node1 ~]# cat /proc/net/nf_conntrack | grep -E \"src=10.130.9.154.*dst=172.30.107.57.*dport=8080.*src=10.131.8.206\" ipv4 2 tcp 6 431986 ESTABLISHED src=10.130.9.154 dst=172.30.107.57 sport=80 dport=8080 src=10.131.8.206 dst=10.130.8.1 sport=8080 dport=80 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=0 use=2   Pod to External 数据包依然首先通过veth-pair送往 OVS 网桥br0，随后便进入了br0上的 OpenFlow 流表：\n table=0, n_packets=837268653828, n_bytes=331648403594327, priority=100,ip actions=goto_table:20 table=20, n_packets=613807687, n_bytes=220557571042, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0-\u003eNXM_NX_REG0[],goto_table:21 table=21, n_packets=837674296060, n_bytes=331665441915651, priority=0 actions=goto_table:30 table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100 table=100, n_packets=761732023982, n_bytes=282091648536325, priority=0 actions=output:2  数据包从tun0端口流出后进入节点的路由表及 iptables 规则：\n1 2 3 4 5 6 7 8  Chain POSTROUTING (policy ACCEPT 2910 packets, 299K bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0 2940M 289G OPENSHIFT-MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* rules for masquerading OpenShift traffic */ Chain OPENSHIFT-MASQUERADE (1 references) pkts bytes target prot opt in out source destination 322M 19G MASQUERADE all -- * * 10.128.0.0/14 0.0.0.0/0 /* masquerade pod-to-service and pod-to-external traffic */   访问集群外部显然需要通过节点的默认网关，因此数据包将从节点网卡eth0送出。而在POSTROUTING链中，数据包的源地址由 Pod IP 转换为了eth0的 IP 10.122.28.7。完整流程如下图所示（图中的 Router 指的是路由器而非 Openshift 中的概念）：\nFuture Work  本文并未涉及 External to Pod 的场景，它是如何实现的？我们都知道 Openshift 是通过 Router（HAProxy）来暴露集群内部服务的，那么数据包在传输过程中的 NAT 操作是怎样进行的？ 除了本文提到的几种网络接口外，Openshift 节点上还存在着ovs-system和vxlan_sys_4789。它们的作用是什么？ Openshift 4.X 版本的网络模型与本文实验用的 3.6 版本相比有那些变化？  参考文献 OpenFlow - Wikipedia\nOVS 在云项目中的使用\nOpenShift SDN - OpenShift Container Platform 3.11\n理解 OpenShift（3）：网络之 SDN\n[译] 深入理解 iptables 和 netfilter 架构\nLinux Netfilter: How does connection tracking track connections changed by NAT?\n","description":"","tags":["Openshift","Network","CNI","Open vSwitch"],"title":"对 Openshift SDN 网络模型的一些探索","uri":"/posts/explorations-on-the-openshift-sdn-network-model/"},{"categories":null,"content":"前言 A Guide to the Kubernetes Networking Model 一文生动形象地介绍了 Kubernetes 中的网络模型，然而受篇幅所限，作者并没有对 Pod 跨节点通信时数据包在节点之间传递的细节进行过多讨论。\n我们已经知道，Docker 使用端口映射的方式实现不同主机间容器的通信，Kubernetes 中同样也有 hostPort 的概念。但是当节点和 Pod 的数量上升后，手动管理节点上绑定的端口是十分困难的，这也是NodePort类型的 Service 的缺点之一。而一旦 Pod 不再“借用”节点的 IP 和端口来暴露自身的服务，就不得不面临一个棘手的问题：Pod 的本质是节点中的进程，节点外的物理网络设备（交换机/路由器）并不知晓 Pod 的存在。它们在接收目的地址为 Pod IP 的数据包时，无法完成进一步的传输工作。\n为此我们需要使用一些 CNI（Container Network Interface）插件来完善 Kubernetes 集群的网络模型，这种新型的网络设计理念称为 SDN（Software-defined Networking）。根据 SDN 实现的层级，我们可以将其分为 Underlay Network 和 Overlay Network：\n Overlay 网络允许设备跨越底层物理网络（Underlay Network）进行通信，而底层却并不知晓 Overlay 网络的存在。Underlay 网络是专门用来承载用户 IP 流量的基础架构层，它与 Overlay 网络之间的关系有点类似物理机和虚拟机。Underlay 网络和物理机都是真正存在的实体，它们分别对应着真实存在的网络设备和计算设备，而 Overlay 网络和虚拟机都是依托在下层实体使用软件虚拟出来的层级。\n Underlay Network 利用 Underlay Network 实现 Pod 跨节点通信，既可以只依赖 TCP/IP 模型中的二层协议，也可以使用三层。但无论哪种实现方式，都必须对底层的物理网络有所要求。\n二层 如图所示，Pod 与节点的 IP 地址均处于同一网段。当 Pod1 向另一节点上的 Pod2 发起通信时，数据包首先通过veth-pair和cbr0送往 Node1 的网卡。由于目的地址 10.86.44.4 与 Node1 同网段，因此 Node1 将通过 ARP 广播请求 10.86.44.4 的 MAC 地址。\nCNI 插件不仅为 Pod 分配 IP 地址，它还会将每个 Pod 所在的节点信息下发给 SDN 交换机。这样当 SDN 交换机接收到 ARP 请求时，将会答复 Pod2 所在节点 Node2 的 MAC 地址，数据包也就顺利地送到了 Node2 上。\n阿里云 Terway 模式的 ACK 服务使用的便是这种网络模型，只不过 Pod 间通信使用的 SDN 交换机不再是节点的交换机（下图中的 Node VSwitch），而是单独创建的 Pod VSwitch：\n三层 如图所示，Pod 与节点的 IP 地址不再处于同一网段。当 Pod1 向另一节点上的 Pod2 发起通信时，数据包首先通过veth-pair和cbr0进入宿主机内核的路由表（Routing Table）。CNI 插件在该表中添加了若干条路由规则，如目的地址为 Pod2 IP 的网关为 Node2 的 IP。这样数据包的目的 MAC 地址就变为了 Node2 的 MAC 地址，它将会通过交换机发送到 Node2 上。\n由于这种实现方式基于三层协议，因此不要求两节点处于同一网段。不过需要将目的地址为 Pod2 IP 的网关设置为 SDN 路由器的 IP，且该路由器能够知晓目的 Pod 所在的节点。这样数据包的目的 MAC 地址就会首先变为 SDN 路由器的 MAC 地址，经过路由器后再变为 Node2 的 MAC 地址：\n通过上面的讨论我们发现，想要实现三层的 Underlay 网络，需要在多个节点间下发和同步路由表。于是很容易想到用于交换路由信息的 BGP（Border Gateway Protocol）协议：\n 边界网关协议（英语：Border Gateway Protocol，缩写：BGP）是互联网上一个核心的去中心化自治路由协议。它通过维护 IP 路由表或“前缀”表来实现自治系统（AS）之间的可达性，属于矢量路由协议。BGP 不使用传统的内部网关协议（IGP）的指标，而使用基于路径、网络策略或规则集来决定路由。因此，它更适合被称为矢量性协议，而不是路由协议。\n 对于 Calico 的 BGP 模式来说，我们可以把集群网络模型视为在每个节点上都部署了一台虚拟路由器。路由器可以与其他节点上的路由器通过 BGP 协议互通，它们称为一对 BGP Peers。Calico 的默认部署方式为 Full-mesh，即创建一个完整的内部 BGP 连接网，每个节点上的路由器均互为 BGP Peers。这种方式仅适用于 100 个节点以内的中小型集群，在大型集群中使用的效率低下。而 Route reflectors 模式则将部分节点作为路由反射器，其他节点上的路由器只需与路由反射器互为 BGP Peers。这样便可以大大减少集群中 BGP Peers 的数量，从而提升效率。\nOverlay Network Overlay 网络可以通过多种协议实现，但通常是对 IP 数据包进行一层外部封装（Encapsulation）。这样底层的 Underlay 网络便只会看到外部封装的数据，而无需处理内部的原有数据。Overlay 网络发送数据包的方式取决于其类型和使用的协议，如基于 VxLAN 实现 Overlay 网络，数据包将被外部封装后以 UDP 协议进行发送：\nOverlay 网络的实现并不依赖于底层物理网络设备，因此我们就以一个两节点不处于同一网段且 Pod 与节点亦处于不同网段的例子来说明 Overlay 网络中的数据包传递过程。集群网络使用 VxLAN 技术组建，虚拟网络设备 VTEP（Virtual Tunnel End Point）将会完成数据包的封装和解封操作。\nNode1 上的 VTEP 收到 Pod1 发来的数据包后，首先会在本地的转发表中查找目的 Pod 所在节点的 IP，即 192.168.1.100。随后它将本机 IP 地址 10.86.44.2、Node2 的 IP 地址 192.168.1.100 和 Pod1 的 VNID（VxLAN Network Identifier）封装在原始数据包外，从 Node1 的网络接口 eth0 送出。由于新构建的数据包源/目的地址均为节点的 IP，因此外部的路由器可以将其转发到 Node2 上。Node2 中的 VTEP 在接收到数据包后会首先进行解封，若源 VNID（Pod1 的 VNID）与目的 VNID（Pod2 的 VNID）一致，便会根据原始数据包中的目的地址 172.100.1.2 将其发送到 Pod2 上。此处的 VNID 检查，主要是为了实现集群的网络策略管理和多租户隔离。\n通过对上述几种 SDN 网络模型的讨论，我们发现只有 Overlay 网络需要对数据包进行封装和解封，因此它的性能相比于 Underlay 网络较差。但 Overlay 网络也有以下优点：\n 对底层网络设备的依赖性最小。即使 Pod 所在的节点发生迁移，依然可以通过 Overlay 网络与原集群实现二层网络的互通； VNID 共有 24 位，因此可以构造出约 1600 万个互相隔离的虚拟网络。  Future Work  除了 VxLAN 以外，还有哪些技术可以实现 Overlay 网络？它们是怎样传输数据的呢？ 本文在讨论 Underlay 网络时提到了 Terway 和 Calico，那么有哪些使用 Overlay 网络的 CNI 插件呢？ 更新：我在 对 Openshift SDN 网络模型的一些探索 中介绍了基于 Overlay 网络的 Open vSwitch； 近年来发展迅速的 Cilium 是怎样实现 SDN 网络的？它所依赖的 eBPF 技术又是什么？  参考文献 Software-defined networking - Wikipedia\nAbout Kubernetes Networking\n使用 Terway 网络插件\n边界网关协议 - Wikipedia\nConfigure BGP peering - Calico\n为什么集群需要 Overlay 网络\n","description":"","tags":["Kubernetes","Network","CNI"],"title":"Kubernetes Pod 是如何跨节点通信的？","uri":"/posts/how-kubernetes-pods-communicate-across-nodes/"},{"categories":null,"content":"前言 通常，Kafka 中的每个 Partiotion 中有多个副本 (Replica) 以实现高可用。想象一个场景，Consumer 正在消费 Leader 中 Offset=10 的数据，而此时 Follower 中只同步到 Offset=8。那么当 Leader 所在的 Broker 宕机后，当前 Follower 经选举成为新的 Leader，Consumer 再次消费时便会报错。因此，Kafka 引入了 HW（High Watermark，高水位）机制来保证副本数据的可靠性和一致性。\nHW 是什么？ HW 定义了消息的可见性，即标识 Partition 中的哪些消息是可以被 Consumer 消费的，只有小于 HW 值的消息才被认为是已备份或已提交的（committed）。而 LEO（Log End Offset）则表示副本写入下一条消息的 Offset，因此同一副本的 HW 值永远不会大于其 LEO 值。\n当集群中副本所在的 Broker 发生故障而后恢复时，副本先将数据截断（Truncation）到其 HW 处（LEO 等于 HW），然后再开始向 Leader 同步数据。\nHW 的更新机制 每一个副本都保存了其 HW 值和 LEO 值，即 Leader HW（实际上也是 Partition HW）、Leader LEO 和 Follower HW、Follower LEO。而 Leader 所在的 Broker 上还保存了其他 Follower 的 LEO 值，称为 Remote LEO。上述几个值的更新流程如下：\n如图所示，当 Producer 向 log 文件写入数据时，Leader LEO 首先被更新。而 Remote LEO 要等到 Follower 向 Leader 发送同步请求（Fetch）时，才会根据请求携带的当前 Follower LEO 值更新。随后，Leader 计算所有副本 LEO 的最小值，将其作为新的 Leader HW。考虑到 Leader HW 只能单调递增，因此还增加了一个 LEO 最小值与当前 Leader HW 的比较，防止 Leader HW 值降低（max[Leader HW, min(All LEO)]）。\nFollower 在接收到 Leader 的响应（Response）后，首先将消息写入 log 文件中，随后更新 Follower LEO。由于 Response 中携带了新的 Leader HW，Follower 将其与刚刚更新过的 Follower LEO 相比较，取最小值作为 Follower HW（min(Follower LEO, Leader HW)）。\n举例来说，如果一开始 Leader 和 Follower 中没有任何数据，即所有值均为 0。那么当 Prouder 向 Leader 写入第一条消息，上述几个值的变化顺序如下：\n    Leader LEO Remote LEO Leader HW Follower LEO Follower HW     Producer Write 1 0 0 0 0   Follower Fetch 1 0 0 0 0   Leader Update HW 1 0 0 0 0   Leader Response 1 0 0 1 0   Follower Update HW 1 0 0 1 0   Follower Fetch 1 1 0 1 0   Leader Update HW 1 1 1 1 0   Leader Response 1 1 1 1 0   Follower Update HW 1 1 1 1 1    HW 的隐患 通过上面的表格我们发现，Follower 往往需要进行两次 Fetch 请求才能成功更新 HW。Follower HW 在某一阶段内总是落后于 Leader HW，因此副本在根据 HW 值截取数据时将有可能发生数据的丢失或不一致。\n图中两副本的 LEO 均为 2，但 Leader 副本 B 上的 HW 为 2，Follower 副本 A 上的 HW 为 1。正常情况下，副本 A 将在接收 Leader Response 后根据 Leader HW 更新其 Follower HW 为 2。但假如此时副本 A 所在的 Broker 重启，它会把 Follower LEO 修改为重启前自身的 HW 值 1，因此数据 M1（Offset=1）被截断。当副本 A 重新向副本 B 发送同步请求时，如果副本 B 所在的 Broker 发生宕机，副本 A 将被选举成为新的 Leader。即使副本 B 所在的 Broker 能够成功重启且其 LEO 值依然为 2，但只要它向当前 Leader（副本 A）发起同步请求后就会更新其 HW 为 1（计算min(Follower LEO, Leader HW)），数据 M1（Offset=1）随即被截断。如果min.insync.replicas参数为 1，那么 Producer 不会因副本 A 没有同步成功而重新发送消息，M1 也就永远丢失了。\n图中 Leader 副本 B 写入了两条数据 M0 和 M1，Follower 副本 A 只写入了一条数据 M0。此时 Leader HW 为 2，Follower HW 为 1。如果在 Follower 同步第二条数据前，两副本所在的 Broker 均发生重启且副本 B 所在的 Broker 先重启成功，那么副本 A 将成为新的 Leader。这时 Producer 向其写入数据 M2，副本 A 作为集群中的唯一副本，更新其 HW 为 2。当副本 B 所在的 Broker 重启后，它将向当前的 Leader 副本 A 同步数据。由于两者的 HW 均为 2，因此副本 B 不需要进行任何截断操作。在这种情况下，副本 B 中的数据为重启前的 M0 和 M1，副本 A 中的数据却是 M0 和 M2，副本间的数据出现了不一致。\nLeader Epoch Kakfa 引入 Leader Epoch 后，Follower 就不再参考 HW，而是根据 Leader Epoch 信息来截断 Leader 中不存在的消息。这种机制可以弥补基于 HW 的副本同步机制的不足，Leader Epoch 由两部分组成：\n Epoch：一个单调增加的版本号。每当 Leader 副本发生变更时，都会增加该版本号。Epoch 值较小的 Leader 被认为是过期 Leader，不能再行使 Leader 的权力； 起始位移（Start Offset）：Leader 副本在该 Epoch 值上写入首条消息的 Offset。  举例来说，某个 Partition 有两个 Leader Epoch，分别为 (0, 0) 和 (1, 100)。这意味该 Partion 历经一次 Leader 副本变更，版本号为 0 的 Leader 从 Offset=0 处开始写入消息，共写入了 100 条。而版本号为 1 的 Leader 则从 Offset=100 处开始写入消息。\n每个副本的 Leader Epoch 信息既缓存在内存中，也会定期写入消息目录下的 leaderer-epoch-checkpoint 文件中。当一个 Follower 副本从故障中恢复重新加入 ISR 中，它将：\n 向 Leader 发送 LeaderEpochRequest，请求中包含了 Follower 的 Epoch 信息； Leader 将返回其 Follower 所在 Epoch 的 Last Offset； 如果 Leader 与 Follower 处于同一 Epoch，那么 Last Offset 显然等于 Leader LEO； 如果 Follower 的 Epoch 落后于 Leader，则 Last Offset 等于 Follower Epoch + 1 所对应的 Start Offset。这可能有点难以理解，我们还是以 (0, 0) 和 (1, 100) 为例进行说明：Offset=100 的消息既是 Epoch=1 的 Start Offset，也是 Epoch=0 的 Last Offset； Follower 接收响应后根据返回的 Last Offset 截断数据； 在数据同步期间，只要 Follower 发现 Leader 返回的 Epoch 信息与自身不一致，便会随之更新 Leader Epoch 并写入磁盘。  在刚刚介绍的数据丢失场景中，副本 A 所在的 Broker 重启后根据自身的 HW 将数据 M1 截断。而现在，副本 A 重启后会先向副本 B 发送一个请求（LeaderEpochRequest）。由于两副本的 Epoch 均为 0，副本 B 返回的 Last Offset 为 Leader LEO 值 2。而副本 A 上并没有 Offset 大于等 2 的消息，因此无需进行数据截断，同时其 HW 也会更新为 2。之后副本 B 所在的 Broker 宕机，副本 A 成为新的 Leader，Leader Epoch 随即更新为 (1, 2)。当副本 B 重启回来并向当前 Leader 副本 A 发送 LeaderEpochRequest，得到的 Last Offset 为 Epoch=1 对应的 Start Offset 值 2。同样，副本 B 中消息的最大 Offset 值只有 1，因此也无需进行数据截断，消息 M1 成功保留了下来。\n在刚刚介绍的数据不一致场景中，由于最后两副本 HW 值相等，因此没有将不一致的数据截断。而现在，副本 A 重启后并便会更新 Leader Epoch 为 (1, 1)，同时也会更新其 HW 值为 2。副本 B 重启后向当前 Leader 副本 A 发送 LeaderEpochRequest，得到的 Last Offset 为 Epoch=1 对应的 Start Offset 值 1，因此截断 Offset=1 的消息 M1。这样只要副本 B 再次发起请求同步消息 M2，两副本的数据便可以保持一致。\n值得一提的是，Leader Epoch 机制在min.insync.replicas参数为 1 且unclean.leader.election.enabled参数为true时依然无法保证数据的可靠性。感兴趣的读者可以阅读 KIP-101 - Alter Replication Protocol to use Leader Epoch rather than High Watermark for Truncation 文中的附录部分。\n参考文献 KIP-101 - Alter Replication Protocol to use Leader Epoch rather than High Watermark for Truncation\n","description":"","tags":["Kafka"],"title":"Kafka 是如何同步副本的？","uri":"/posts/how-does-kafka-synchronize-replicas/"}]