<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><title type="text">Inspire Hub</title><subtitle type="html">MemE is a powerful and highly customizable GoHugo theme for personal blogs.</subtitle><updated>2021-10-26T13:35:34+00:00</updated><id>/</id><link rel="alternate" type="text/html" href="/"/><link rel="self" type="application/atom+xml" href="/atom.xml"/><author><name>koktlzz</name><uri>/</uri><email>koktlgwr@gmail.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><generator uri="https://gohugo.io/" version="0.88.1">Hugo</generator><entry><title type="text">通过安装 CoreOS 系统了解 Linux 启动流程</title><link rel="alternate" type="text/html" href="/posts/linux-boot-intro/"/><id>/posts/linux-boot-intro/</id><updated>2021-10-25T22:18:43+08:00</updated><published>2021-10-20T00:25:25+08:00</published><author><name>koktlzz</name><uri>/</uri><email>koktlgwr@gmail.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">前言 OpenShift 4.X 版本要求安装在操作系统为 CoreOS 的机器上，因此 官方文档 给出了使用 PXE 或 IPXE 引导 CoreOS 系统的方法。我们可以参考其操作流程，将一台 CentOS 7.X 的机器改写为 CoreOS 系统，步骤如下：
从 镜像下载页 下载安装所需版本的 kernel、initramfs 和 rootfs 文件，并将 rootfs 和点火文件（*.ign）上传到自建的 HTTP 服务器上；</summary><content type="html">&lt;h2 id="前言">前言&lt;/h2>
&lt;p>OpenShift 4.X 版本要求安装在操作系统为 CoreOS 的机器上，因此 &lt;a href="https://docs.openshift.com/container-platform/4.6/installing/installing_bare_metal/installing-restricted-networks-bare-metal.html#installation-user-infra-machines-pxe_installing-restricted-networks-bare-metal">官方文档&lt;/a> 给出了使用 PXE 或 IPXE 引导 CoreOS 系统的方法。我们可以参考其操作流程，将一台 CentOS 7.X 的机器改写为 CoreOS 系统，步骤如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>从 &lt;a href="https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.6/?extIdCarryOver=true&amp;amp;sc_cid=701f2000001Css5AAC">镜像下载页&lt;/a> 下载安装所需版本的 kernel、initramfs 和 rootfs 文件，并将 rootfs 和点火文件（*.ign）上传到自建的 HTTP 服务器上；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>将 kernel 和 initramfs 文件拷贝到 CentOS 7.X 机器的 /boot 目录下；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据需求修改 /boot/grub2 目录下的 grub.cfg 文件；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>重启机器。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>对于操作系统初学者（比如我）来说，很难想象仅依靠添加和修改文件就能改变一台计算机的操作系统。为了解其实现原理，我们将对 Linux 的启动流程进行讨论，并从中说明上述操作是如何影响操作系统的。&lt;/p>
&lt;h2 id="linux-启动流程">Linux 启动流程&lt;/h2>
&lt;p>启动一台 Linux 机器的过程可以分为两个部分：Boot 和 Startup。其中，Boot 起始于计算机启动，在内核初始化完成且 systemd 进程开始加载后结束。紧接着， Startup 接管任务，使计算机达到一个用户可操作的状态。&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202110171642.jpeg" alt="202110171642">&lt;/p>
&lt;h2 id="boot-阶段">Boot 阶段&lt;/h2>
&lt;p>如上图所示，Boot 阶段又可以细分为三个部分：&lt;/p>
&lt;ul>
&lt;li>BIOS POST&lt;/li>
&lt;li>Boot Loader&lt;/li>
&lt;li>内核初始化&lt;/li>
&lt;/ul>
&lt;h3 id="bios-post">BIOS POST&lt;/h3>
&lt;p>开机自检（Power On Self Test，POST）是 &lt;a href="https://zh.wikipedia.org/wiki/BIOS">基本输入输出系统&lt;/a>（Basic I/O System，BIOS）的一部分，也是启动 Linux 机器的第一个步骤。其工作对象是计算机硬件，因此对于任何操作系统都是相同的。&lt;strong>POST 检查硬件的基本可操作性&lt;/strong>，若失败则 Boot 过程将会被终止。&lt;/p>
&lt;p>POST 检查完毕后会发出一个 BIOS 中断调用 &lt;a href="https://en.wikipedia.org/wiki/INT_13H">INT 13H&lt;/a>，它将在任何可连接且可引导的磁盘上搜索含有有效引导记录的引导扇区（Boot Sector），通常是 &lt;a href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E5%BC%95%E5%AF%BC%E8%AE%B0%E5%BD%95">主引导扇区&lt;/a>。引导扇区中的主引导记录（Master Boot Record，MBR）将被加载到 RAM 中，然后控制权就会转移到其手中。&lt;/p>
&lt;h3 id="boot-loader">Boot Loader&lt;/h3>
&lt;p>大多数 Linux 发行版使用三种 Boot Loader 程序：GRUB1、GRUB2 和 LILO，其中 GRUB2 是最新且使用最为广泛的。GRUB2 代表“GRand Unified Bootloader, version 2”，&lt;strong>它能够定位操作系统内核并将其加载到内存中&lt;/strong>。GRUB2 还允许用户选择从几种不同的内核中引导计算机，如果更新的内核版本出现兼容性问题，我们就可以恢复到先前内核版本。&lt;/p>
&lt;p>GRUB1 的引导过程可以分为三个阶段：stage 1、stage 1.5 和 stage 2。虽然 GRUB2 中并没有 stage 的概念，但两者的工作方式基本相同。为了方便说明，我们在讨论 GRUB2 时将沿用 GRUB1 中 stage 的说法。&lt;/p>
&lt;h4 id="stage-1">stage 1&lt;/h4>
&lt;p>上文提到，BIOS 中断调用会定位主引导扇区，其结构如下图所示：&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/NoteImg@main/20211015161614.png" alt="20211015161614">&lt;/p>
&lt;p>主引导记录首部的引导代码便是 stage 1 文件 boot.img，它和 stage 1.5 文件 core.img 均位于 /boot/grub2/i386-pc 目录下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="o">[&lt;/span>root@bastion ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># du -b /boot/grub2/i386-pc/*.img &lt;/span>
&lt;span class="m">512&lt;/span> /boot/grub2/i386-pc/boot.img
&lt;span class="m">26664&lt;/span> /boot/grub2/i386-pc/core.img
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>它的作用是检查分区表是否正确，然后&lt;strong>定位和加载 stage 1.5&lt;/strong> 文件。446 字节的 boot.img 放不下能够识别文件系统的代码，只能通过计算扇区的偏移量来寻找，因此 core.img 必须位于主引导记录和驱动器的第一个分区（partition）之间。第一个分区从扇区 63 开始，与位于扇区 0 的主引导记录之间有 62 个扇区（每个 512 字节），有足够的空间存储大小不足 30000 字节的 core.img 文件。当 core.img 文件加载到 RAM 后，控制权也随之转移。&lt;/p>
&lt;h4 id="stage-15">stage 1.5&lt;/h4>
&lt;p>相比于只能读取原始扇区的 LILO，GRUB1 和 GRUB2 均可识别文件系统，这依赖于 stage 1.5 文件中内置的文件系统驱动程序。如果你拥有一台仍然使用 GRUB1 引导的 CentOS 6.X 机器，那么便可以在 /boot/grub/ 目录下找到这些适配不同文件系统的 stage 1.5 文件：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="o">[&lt;/span>root@centos6.5 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># du -b /boot/grub/* | grep stage1_5&lt;/span>
&lt;span class="m">13380&lt;/span> /boot/grub/e2fs_stage1_5
&lt;span class="m">12620&lt;/span> /boot/grub/fat_stage1_5
&lt;span class="m">11748&lt;/span> /boot/grub/ffs_stage1_5
&lt;span class="m">11756&lt;/span> /boot/grub/iso9660_stage1_5
&lt;span class="m">13268&lt;/span> /boot/grub/jfs_stage1_5
&lt;span class="m">11956&lt;/span> /boot/grub/minix_stage1_5
&lt;span class="m">14412&lt;/span> /boot/grub/reiserfs_stage1_5
&lt;span class="m">12024&lt;/span> /boot/grub/ufs2_stage1_5
&lt;span class="m">11364&lt;/span> /boot/grub/vstafs_stage1_5
&lt;span class="m">13964&lt;/span> /boot/grub/xfs_stage1_5
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>GRUB2 中的 core.img 不仅整合了上述文件系统驱动，还新增了菜单处理等模块，这也是其优于 GRUB1 的地方。我们可以在 &lt;a href="https://www.gnu.org/software/grub/manual/grub/html_node/Images.html#Images">GNU GRUB Manual 2.06: Images&lt;/a> 中找到对各种 GRUB 镜像文件的详细介绍。&lt;/p>
&lt;p>既然 core.img 文件可以识别文件系统，那么它就能够根据安装时确定的系统路径&lt;strong>定位和加载 stage 2&lt;/strong> 文件。同样，当 stage 2 文件加载到 RAM 后，控制权也随之转移。&lt;/p>
&lt;h4 id="stage-2">stage 2&lt;/h4>
&lt;p>stage 2 文件并非是一个 .img 的镜像，而是一些运行时内核模块：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="o">[&lt;/span>root@bastion ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ls /boot/grub2/i386-pc/ | grep .mod | head&lt;/span>
acpi.mod
adler32.mod
affs.mod
afs.mod
ahci.mod
all_video.mod
aout.mod
appendedsig.mod
appended_signature_test.mod
archelp.mod
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>它们的任务是根据 grub.cfg 文件的配置&lt;strong>定位和加载内核文件&lt;/strong>，然后将控制权转交给 Linux 内核。grub.cfg 文件存放在 /boot/grub2 目录下：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="o">[&lt;/span>root@bastion ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># head /boot/grub2/grub.cfg -n 5&lt;/span>
&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># DO NOT EDIT THIS FILE&lt;/span>
&lt;span class="c1">#&lt;/span>
&lt;span class="c1"># It is automatically generated by grub2-mkconfig using templates&lt;/span>
&lt;span class="c1"># from /etc/grub.d and settings from /etc/default/grub&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>通过该文件的注释我们可以知道，它实际上是由 grub2-mkconfig 命令使用 /etc/grub.d 目录下的一些模板文件并根据 /etc/default/grub 文件中的设置生成的：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="o">[&lt;/span>root@bastion ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ls /etc/grub.d/&lt;/span>
00_header 00_tuned 01_users 10_linux 20_linux_xen 20_ppc_terminfo 30_os-prober 40_custom 41_custom README
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>40_custom 和 41_custom 文件常用于用户对 GRUB2 配置的修改，实际上我们对机器的操作也是从这里开始的。为了让 GRUB2 在机器启动时选择 CoreOS 系统内核而非默认的 CentOS，需要在原始 40_custom 文件末尾添加如下内容：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">menuentry &lt;span class="s1">&amp;#39;coreos&amp;#39;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nb">set&lt;/span> &lt;span class="nv">root&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;hd0,msdos1&amp;#39;&lt;/span>
linux16 /rhcos-live-kernel-x86_64 coreos.inst&lt;span class="o">=&lt;/span>yes coreos.inst.install_dev&lt;span class="o">=&lt;/span>vda rd.neednet&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span> &lt;span class="nv">console&lt;/span>&lt;span class="o">=&lt;/span>tty0 &lt;span class="nv">console&lt;/span>&lt;span class="o">=&lt;/span>ttyS0 coreos.live.rootfs_url&lt;span class="o">=&lt;/span>http://&lt;span class="o">{{&lt;/span>HTTP-Server-Path&lt;span class="o">}}&lt;/span>/rhcos-live-rootfs.x86_64.img coreos.inst.ignition_url&lt;span class="o">=&lt;/span>http://&lt;span class="o">{{&lt;/span>HTTP-Server-Path&lt;span class="o">}}&lt;/span>/master.ign &lt;span class="nv">ip&lt;/span>&lt;span class="o">=&lt;/span>dhcp
initrd16 /rhcos-live-initramfs.x86_64.img
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>所示的 Menuentry 由三条 Shell 命令组成：&lt;/p>
&lt;ul>
&lt;li>&lt;code>set root='hd0,msdos1'&lt;/code>&lt;/li>
&lt;li>&lt;code>linux16 /rhcos-live-kernel-x86_64 ...&lt;/code>&lt;/li>
&lt;li>&lt;code>initrd16 /rhcos-live-initramfs.x86_64.img&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>第一条命令指定了 GRUB2 的根目录，也就是 /boot 所在分区在计算机硬件上的位置。既然我们已经将内核文件拷贝到了 /boot 目录下，那么能够识别文件系统的 GRUB2 便可以定位和加载它。本例中&lt;code>hd&lt;/code>代表硬盘（hard drive），&lt;code>0&lt;/code>代表第一块硬盘，&lt;code>mosdos&lt;/code>代表分区格式，&lt;code>1&lt;/code> 代表第一个分区。详细的硬件命名规范见 &lt;a href="https://www.gnu.org/software/grub/manual/grub/grub.html#Naming-convention">Naming Convention&lt;/a>。&lt;/p>
&lt;p>第二条命令将从&lt;code>rhcos-live-kernel-x86_64&lt;/code>（CoreOS 系统的内核文件）中以 16 位模式加载 Linux 内核映像，并通过&lt;code>coreos.live.rootfs_url&lt;/code>和&lt;code>coreos.inst.ignition_url&lt;/code>参数指定根文件系统（rootfs）的镜像文件和点火文件的下载链接。&lt;code>ip=dhcp&lt;/code>代表该计算机网络将由 DHCP 服务器动态配置，也可以按&lt;code>ip={{HostIP}}::{{Gateway}}:{{Genmask}}:{{Hostname}}::none nameserver={{DNSServer}}&lt;/code>的格式写入静态配置。&lt;/p>
&lt;p>第三条命令将从&lt;code>rhcos-live-initramfs.x86_64.img&lt;/code>中加载 RAM Filesystem。GRUB2 读取的内核文件实际上只包含了内核的核心模块，缺少硬件驱动模块的它无法完成 rootfs 的挂载。然而这些硬件驱动模块位于 /lib/modules/$(uname -r)/kernel/ 目录下，必须在 rootfs 挂载完毕后才能被识别和加载。为了解决这一问题，initramfs（前身为 initrd）应运而生。它是一个包含了必要驱动模块的临时 rootfs，内核可以从中加载所需的驱动程序。待真正的 rootfs 挂载完毕后，它便会从内存中移除。&lt;/p>
&lt;p>除此之外我们还需要将 /etc/default/grub 文件中的 &lt;a href="https://www.gnu.org/software/grub/manual/grub/grub.html#Simple-configuration">GRUB_DEFAULT=saved&lt;/a> 修改为 GRUB_DEFAULT=&amp;ldquo;coreos&amp;rdquo;，使其与 40_custom 文件中的&lt;code>menuentry 'coreos'&lt;/code>对应。最后使用命令&lt;code>grub2-mkconfig -o /boot/grub2/grub.cfg&lt;/code>来重新生成一份 grub.cfg 文件，这样计算机重启后 GRUB2 就会根据我们的配置去加载 CoreOS 系统的内核了。&lt;/p>
&lt;p>至此我们已经明白了为什么“仅依靠添加和修改文件就能改变一台计算机的操作系统”，但计算机想要达到用户可操作状态还远不止于此。让我们再来看看内核被加载到内存后发生了什么。&lt;/p>
&lt;h3 id="内核初始化">内核初始化&lt;/h3>
&lt;p>不同内核及其相关文件位于 /boot 目录中，均以 vmlinuz 开头：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="o">[&lt;/span>root@bastion ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ls /boot/ | grep vmlinuz&lt;/span>
vmlinuz-0-rescue-20210623110808105647395700239158
vmlinuz-4.18.0-305.12.1.el8_4.x86_64
vmlinuz-4.18.0-305.3.1.el8.x86_64
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>内核通过压缩自身来节省存储空间，所以当选定的内核被加载到内存中后，它首先需要进行解压缩（extracting）。一旦解压完成，内核便会开始&lt;strong>加载 systemd 并将控制权移交给它&lt;/strong>。&lt;/p>
&lt;h2 id="startup-阶段">Startup 阶段&lt;/h2>
&lt;p>systemd 是所有进程之父，它负责&lt;strong>使计算机达到可以完成生产工作的状态&lt;/strong>。其功能比过去的 init 程序要丰富得多，包括挂载文件系统、启动和管理计算机所需的系统服务。当然你也可以将一些应用（如 Docker）以 systemd 的方式启动，但它们与 Linux 的启动无关，因此不在本文的讨论范围之内。&lt;/p>
&lt;p>首先，systemd 根据 /etc/fstab 文件中的配置挂载文件系统。然后读取 /etc 目录下的配置文件，包括其自身的配置文件 /etc/systemd/system/default.target。该文件指定了 systemd 需要引导计算机到达的最终目标和状态，实际上是一个软链接：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="o">[&lt;/span>root@bastion ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ls /etc/systemd/system/default.target -l&lt;/span>
lrwxrwxrwx. &lt;span class="m">1&lt;/span> root root &lt;span class="m">37&lt;/span> Oct &lt;span class="m">17&lt;/span> &lt;span class="m">2019&lt;/span> /etc/systemd/system/default.target -&amp;gt; /lib/systemd/system/multi-user.target
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>在我使用的 bastion 服务器上，它指向的是 multi-user.target；对于带有图形化界面的桌面工作站，它通常指向 graphics.target；而对于单用户模式的机器，它将指向 emergency.target。target 等效于过去 SystemV 中的 &lt;a href="https://zh.wikipedia.org/wiki/%E8%BF%90%E8%A1%8C%E7%BA%A7%E5%88%AB">运行级别&lt;/a>（Runlevel），它提供了别名以实现向后兼容性：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>SystemV Runlevel&lt;/th>
&lt;th>systemd target&lt;/th>
&lt;th>systemd target alias&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>halt.target&lt;/td>
&lt;td>&lt;/td>
&lt;td>在不关闭电源的情况下中止系统。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>poweroff.target&lt;/td>
&lt;td>runlevel0.target&lt;/td>
&lt;td>中止系统并关闭电源。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>s&lt;/td>
&lt;td>emergency.target&lt;/td>
&lt;td>&lt;/td>
&lt;td>单用户模式。 没有服务正在运行，也未挂载文件系统。仅在主控制台上运行一个紧急 Shell，供用户与系统交互。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>rescue.target&lt;/td>
&lt;td>runlevel1.target&lt;/td>
&lt;td>一个基本系统。文件系统已挂载，只运行最基本的服务和主控制台上的紧急 Shell。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>&lt;/td>
&lt;td>runlevel2.target&lt;/td>
&lt;td>多用户模式。虽然还没有网络连接，但不依赖网络的所有非 GUI 服务都已运行。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>multi-user.target&lt;/td>
&lt;td>runlevel3.target&lt;/td>
&lt;td>所有服务都在运行，但只能使用命令行界面（CLI）。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>&lt;/td>
&lt;td>runlevel4.target&lt;/td>
&lt;td>用户自定义&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>graphical.target&lt;/td>
&lt;td>runlevel5.target&lt;/td>
&lt;td>所有服务都在运行，并且可以使用图形化界面（GUI）。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>reboot.target&lt;/td>
&lt;td>runlevel6.target&lt;/td>
&lt;td>重启系统&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>每个 target 都在其配置文件中指定了一组依赖，由 systemd 负责启动。这些依赖是 Linux 达到某个运行级别所必须的服务（service）。换句话说，当一个 target 配置文件中的所有 service 都已成功加载，那么系统就达到了该 target 对应的运行级别。&lt;/p>
&lt;p>下图展示了 systemd 启动过程中各 target 和 service 实现的一般顺序：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell"> cryptsetup-pre.target veritysetup-pre.target
&lt;span class="p">|&lt;/span>
&lt;span class="o">(&lt;/span>various low-level v
API VFS mounts: &lt;span class="o">(&lt;/span>various cryptsetup/veritysetup devices...&lt;span class="o">)&lt;/span>
mqueue, configfs, &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
debugfs, ...&lt;span class="o">)&lt;/span> v &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> cryptsetup.target &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="o">(&lt;/span>various swap &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> remote-fs-pre.target
&lt;span class="p">|&lt;/span> devices...&lt;span class="o">)&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> v
&lt;span class="p">|&lt;/span> v local-fs-pre.target &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="o">(&lt;/span>network file systems&lt;span class="o">)&lt;/span>
&lt;span class="p">|&lt;/span> swap.target &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> v v &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> v &lt;span class="p">|&lt;/span> remote-cryptsetup.target &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="o">(&lt;/span>various low-level &lt;span class="o">(&lt;/span>various mounts and &lt;span class="p">|&lt;/span> remote-veritysetup.target &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> services: udevd, fsck services...&lt;span class="o">)&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> remote-fs.target
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> tmpfiles, random &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> /
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> seed, sysctl, ...&lt;span class="o">)&lt;/span> v &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> /
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> local-fs.target &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> /
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> /
&lt;span class="se">\_&lt;/span>___&lt;span class="p">|&lt;/span>______&lt;span class="p">|&lt;/span>_______________ ______&lt;span class="p">|&lt;/span>___________/ &lt;span class="p">|&lt;/span> /
&lt;span class="se">\ &lt;/span>/ &lt;span class="p">|&lt;/span> /
v &lt;span class="p">|&lt;/span> /
sysinit.target &lt;span class="p">|&lt;/span> /
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> /
______________________/&lt;span class="p">|&lt;/span>&lt;span class="se">\_&lt;/span>____________________ &lt;span class="p">|&lt;/span> /
/ &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="se">\ &lt;/span> &lt;span class="p">|&lt;/span> /
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> /
v v &lt;span class="p">|&lt;/span> v &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> /
&lt;span class="o">(&lt;/span>various &lt;span class="o">(&lt;/span>various &lt;span class="p">|&lt;/span> &lt;span class="o">(&lt;/span>various &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>/
timers...&lt;span class="o">)&lt;/span> paths...&lt;span class="o">)&lt;/span> &lt;span class="p">|&lt;/span> sockets...&lt;span class="o">)&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
v v &lt;span class="p">|&lt;/span> v &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
timers.target paths.target &lt;span class="p">|&lt;/span> sockets.target &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> v &lt;span class="p">|&lt;/span>
v &lt;span class="se">\_&lt;/span>______ &lt;span class="p">|&lt;/span> _____/ rescue.service &lt;span class="p">|&lt;/span>
&lt;span class="se">\|&lt;/span>/ &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
v v &lt;span class="p">|&lt;/span>
basic.target rescue.target &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
________v____________________ &lt;span class="p">|&lt;/span>
/ &lt;span class="p">|&lt;/span> &lt;span class="se">\ &lt;/span> &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
v v v &lt;span class="p">|&lt;/span>
display- &lt;span class="o">(&lt;/span>various system &lt;span class="o">(&lt;/span>various system &lt;span class="p">|&lt;/span>
manager.service services services&lt;span class="o">)&lt;/span> &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> required &lt;span class="k">for&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> graphical UIs&lt;span class="o">)&lt;/span> v v
&lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> multi-user.target
emergency.service &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="p">|&lt;/span>
&lt;span class="p">|&lt;/span> &lt;span class="se">\_&lt;/span>____________ &lt;span class="p">|&lt;/span> _____________/
v &lt;span class="se">\|&lt;/span>/
emergency.target v
graphical.target
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>如上图所示，想要到达到某个 target，其依赖的所有 target 和 service 就必须已完成加载。如实现 sysinit.target，需要先挂载文件系统（local-fs.target）、设置交换文件（swap.target）、初始化 udev （various low-level services）和设置加密服务（cryptsetup.target）等。不过，同一个 target 的不同依赖项可以并行执行。&lt;/p>
&lt;p>当计算机达到 multi-user.target 或 graphical.target 时，它的漫漫启动之路就走到了尽头。但为了满足用户多样的需求，它所面临的挑战其实才刚刚开始。&lt;/p>
&lt;h2 id="future-work">Future Work&lt;/h2>
&lt;p>虽然本文已对 Linux 的启动流程进行了较为深入地讨论，但仍有一些 Topic 值得我们继续探索：&lt;/p>
&lt;ul>
&lt;li>前言提到 RedHat 官方给出了 IPXE/PXE 引导 CoreOS 系统的方法，那么这项技术又是如何实现的呢？&lt;/li>
&lt;li>MBR 只有 446 个字节，可为什么 boot.img 文件却有 512 个字节？&lt;/li>
&lt;li>目前已经有越来越多的计算机使用 UEFI 和 GPT 来代替 BIOS 和 MBR，其优势体现在哪？&lt;/li>
&lt;li>我们该如何理解 systemd 的配置文件？如何使用 systemd 部署我们的应用？&lt;/li>
&lt;/ul>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>&lt;a href="https://docs.openshift.com/container-platform/4.6/installing/installing_bare_metal/installing-restricted-networks-bare-metal.html#installation-user-infra-machines-pxe_installing-restricted-networks-bare-metal">Creating Red Hat Enterprise Linux CoreOS (RHCOS) machines by PXE or iPXE Booting&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://zh.wikipedia.org/wiki/BIOS">BIOS - Wikipedia&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/INT_13H">INT 13H - Wikipedia&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E5%BC%95%E5%AF%BC%E8%AE%B0%E5%BD%95">主引导记录 - Wikipedia&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://opensource.com/article/17/2/linux-boot-and-startup">An Introduction To the Linux Boot and Startup Processes&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.gnu.org/software/grub/manual/grub/grub.html">GNU GRUB Manual 2.06&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://man7.org/linux/man-pages/man7/bootup.7.html">Bootup(7) - Linux manual page&lt;/a>&lt;/p></content><category scheme="/tags/os/" term="OS" label="OS"/><category scheme="/tags/linux/" term="Linux" label="Linux"/><category scheme="/tags/coreos/" term="CoreOs" label="CoreOs"/></entry><entry><title type="text">初探 Open vSwitch</title><link rel="alternate" type="text/html" href="/posts/ovs-intro/"/><id>/posts/ovs-intro/</id><updated>2021-10-26T15:22:26+08:00</updated><published>2020-05-13T09:19:42+01:00</published><author><name>koktlzz</name><uri>/</uri><email>koktlgwr@gmail.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">前言 Openshift SDN 是由 Overlay 网络 OVS（Open vSwitch）建立的，其使用的插件如下：
ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信； ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 oc get netnamspaces 命令查看； ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。 在 Openshift 集群中的节点上，有以下几个网络设备：</summary><content type="html">&lt;h2 id="前言">前言&lt;/h2>
&lt;p>Openshift SDN 是由 Overlay 网络 OVS（Open vSwitch）建立的，其使用的插件如下：&lt;/p>
&lt;ul>
&lt;li>ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信；&lt;/li>
&lt;li>ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 &lt;strong>oc get netnamspaces&lt;/strong> 命令查看；&lt;/li>
&lt;li>ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。&lt;/li>
&lt;/ul>
&lt;p>在 Openshift 集群中的节点上，有以下几个网络设备：&lt;/p>
&lt;ul>
&lt;li>&lt;code>br0&lt;/code>：OpenShift 创建和管理的 OVS 网桥，它会使用 OpenFlow 流表来实现数据包的转发和隔离；&lt;/li>
&lt;li>&lt;code>vxlan0&lt;/code>：VxLAN 隧道端点，即 VTEP（Virtual Tunnel End Point），用于集群内部 Pod 之间的通信；&lt;/li>
&lt;li>&lt;code>tun0&lt;/code>：节点上所有 Pod 的默认网关，用于 Pod 与集群外部和 Pod 与 Service 之间的通信；&lt;/li>
&lt;li>&lt;code>veth&lt;/code>：Pod 通过&lt;code>veth-pair&lt;/code>连接到&lt;code>br0&lt;/code>网桥的端点。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>ovs-ofctl -O OpenFlow13 show br0&lt;/strong> 命令可以查看&lt;code>br0&lt;/code>上的所有端口及其编号：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">[&lt;/span>root@node1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ovs-ofctl -O OpenFlow13 show br0&lt;/span>
OFPT_FEATURES_REPLY &lt;span class="o">(&lt;/span>OF1.3&lt;span class="o">)&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="nv">xid&lt;/span>&lt;span class="o">=&lt;/span>0x2&lt;span class="o">)&lt;/span>: dpid:0000ea00372f1940
n_tables:254, n_buffers:0
capabilities: FLOW_STATS TABLE_STATS PORT_STATS GROUP_STATS QUEUE_STATS
OFPST_PORT_DESC reply &lt;span class="o">(&lt;/span>OF1.3&lt;span class="o">)&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="nv">xid&lt;/span>&lt;span class="o">=&lt;/span>0x3&lt;span class="o">)&lt;/span>:
1&lt;span class="o">(&lt;/span>vxlan0&lt;span class="o">)&lt;/span>: addr:72:23:a0:a9:14:a7
config: &lt;span class="m">0&lt;/span>
state: &lt;span class="m">0&lt;/span>
speed: &lt;span class="m">0&lt;/span> Mbps now, &lt;span class="m">0&lt;/span> Mbps max
2&lt;span class="o">(&lt;/span>tun0&lt;span class="o">)&lt;/span>: addr:62:80:67:c6:38:58
config: &lt;span class="m">0&lt;/span>
state: &lt;span class="m">0&lt;/span>
speed: &lt;span class="m">0&lt;/span> Mbps now, &lt;span class="m">0&lt;/span> Mbps max
8381&lt;span class="o">(&lt;/span>vethd040c191&lt;span class="o">)&lt;/span>: addr:7a:d9:f4:12:94:5f
config: &lt;span class="m">0&lt;/span>
state: &lt;span class="m">0&lt;/span>
current: 10GB-FD COPPER
speed: &lt;span class="m">10000&lt;/span> Mbps now, &lt;span class="m">0&lt;/span> Mbps max
...
LOCAL&lt;span class="o">(&lt;/span>br0&lt;span class="o">)&lt;/span>: addr:76:ab:cf:6f:e1:46
config: PORT_DOWN
state: LINK_DOWN
speed: &lt;span class="m">0&lt;/span> Mbps now, &lt;span class="m">0&lt;/span> Mbps max
OFPT_GET_CONFIG_REPLY &lt;span class="o">(&lt;/span>OF1.3&lt;span class="o">)&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="nv">xid&lt;/span>&lt;span class="o">=&lt;/span>0x5&lt;span class="o">)&lt;/span>: &lt;span class="nv">frags&lt;/span>&lt;span class="o">=&lt;/span>nx-match &lt;span class="nv">miss_send_len&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">0&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>考虑到 Openshift 集群的复杂性，我们分别按以下几种场景分析数据包的流向：&lt;/p>
&lt;ul>
&lt;li>节点内 Pod 互访：Pod to Local Pod&lt;/li>
&lt;li>Pod 跨节点互访：Pod to Remote Pod&lt;/li>
&lt;li>Pod 访问 Service：Pod to Service&lt;/li>
&lt;li>Pod 与集群外部互访：Pod to External&lt;/li>
&lt;/ul>
&lt;p>由于高版本（3.11 以上）的 Openshift 不再以守护进程而是以 Pod 的形式部署 OVS 组件，不方便对 OpenFlow 流表进行查看，因此本文选用的集群版本为 3.6：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">[&lt;/span>root@node1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># oc version &lt;/span>
oc v3.6.173.0.5
kubernetes v1.6.1+5115d708d7
features: Basic-Auth GSSAPI Kerberos SPNEGO
Server https://test-cluster.ocp.koktlzz.com:8443
openshift v3.6.173.0.5
kubernetes v1.6.1+5115d708d7
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>另外，实验用集群并未开启 ovs-multitenant，即未进行多租户隔离。整个集群 Pod 网络是扁平化的，所有 Pod 的 VNID 都为默认值 0。&lt;/p>
&lt;h2 id="pod-to-local-pod">Pod to Local Pod&lt;/h2>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132046.jpeg" alt="202205132046">&lt;/p>
&lt;p>数据包首先通过&lt;code>veth-pair&lt;/code>送往 OVS 网桥&lt;code>br0&lt;/code>，随后便进入了&lt;code>br0&lt;/code>上的 OpenFlow 流表。我们可以用 &lt;strong>ovs-ofctl -O OpenFlow13 dump-flows br0&lt;/strong> 命令查看流表中的规则，同时为了让输出结果更加简洁，略去 cookie 和 duration 的信息：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>table=0, n_packets=62751550874, n_bytes=25344802160312, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.130.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]-&amp;gt;NXM_NX_REG0[],goto_table:10&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=0, n_packets=1081527047094, n_bytes=296066911370148, priority=200,ip,in_port=2 actions=goto_table:30&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=0, n_packets=833353346930, n_bytes=329854403266173, priority=100,ip actions=goto_table:20&lt;/code>&lt;/p>
&lt;p>table0 中关于 IP 数据包的规则主要有三条，其中前两条分别对应流入端口&lt;code>in_port&lt;/code>为 1 号端口&lt;code>vxlan0&lt;/code>和 2 号端口&lt;code>tun0&lt;/code>的数据包。这两条规则的优先级&lt;code>priority&lt;/code>都是 200，因此只有在两者均不符合情况下，才会匹配第三条规则。由于本地 Pod 发出的数据包是由&lt;code>veth&lt;/code>端口进入的，因此将转到 table20；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=20, n_packets=607178746, n_bytes=218036511085, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0-&amp;gt;NXM_NX_REG0[],goto_table:21&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=21, n_packets=833757781068, n_bytes=329871389393381, priority=0 actions=goto_table:30&lt;/code>&lt;/p>
&lt;p>table20 会匹配源地址&lt;code>nw_src&lt;/code>为 10.130.9.154 且流入端口&lt;code>in_port&lt;/code>为 8422 的数据包，随后将 Pod1 的 VNID 0 作为源 VNID 存入寄存器 0 中，经由 table21 转到 table30；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=30, n_packets=21061319859, n_bytes=29568807363654, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100&lt;/code>&lt;/p>
&lt;p>table30 中匹配数据包目的地址&lt;code>nw_dst&lt;/code>的规则有四条，前三条分别对应本节点内 Pod 的 CIDR 网段 10.130.8.0/23、集群内 Pod 的 CIDR 网段 10.128.0.0/14 和 Service 的 ClusterIP 网段 172.30.0.0/16。第四条优先级最低，用于 Pod 对集群外部的访问。由于数据包的目的地址 10.130.9.158 符合第一条规则，且第一条规则的优先级最高，因此将转到 table70；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=70, n_packets=597219981, n_bytes=243824445346, priority=100,ip,nw_dst=10.130.9.158 actions=load:0-&amp;gt;NXM_NX_REG1[],load:0x20ea-&amp;gt;NXM_NX_REG2[],goto_table:80&lt;/code>&lt;/p>
&lt;p>table70 匹配目的地址&lt;code>nw_dst&lt;/code>为 Pod2 IP 10.130.9.158 的数据包，并将 Pod2 的 VNID 0 作为目的 VNID 存入寄存器 1 中。同时端口号&lt;code>0x20ea&lt;/code>被保存到寄存器 2 中，然后转到 table80；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=80, n_packets=1112713040332, n_bytes=293801616636499, priority=200 actions=output:NXM_NX_REG2[]&lt;/code>&lt;/p>
&lt;p>table80 比较寄存器 0 和寄存器 1 中保存的源/目的 VNID。若二者一致，则根据寄存器 2 中保存的端口号将数据包送出。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>端口号&lt;code>0x20ea&lt;/code>是一个十六进制数字，即十进制数 8426。而 Pod2 正是通过 8426 号端口设备&lt;code>vethba48c6de&lt;/code>连接到&lt;code>br0&lt;/code>上，因此数据包便最终通过它流入到了 Pod2 中。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">[&lt;/span>root@node1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ovs-ofctl -O OpenFlow13 show br0 | grep 8426&lt;/span>
8426&lt;span class="o">(&lt;/span>vethba48c6de&lt;span class="o">)&lt;/span>: addr:e6:b2:7e:42:41:91
&lt;span class="o">[&lt;/span>root@node1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ip a | grep vethba48c6de&lt;/span>
8442: vethba48c6de@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu &lt;span class="m">1450&lt;/span> qdisc noqueue master ovs-system state UP
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="pod-to-remote-pod">Pod to Remote Pod&lt;/h2>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202105132042.jpeg" alt="202105132042">&lt;/p>
&lt;h3 id="packet-in-local-pod">Packet in Local Pod&lt;/h3>
&lt;p>数据包依然首先通过&lt;code>veth-pair&lt;/code>送往 OVS 网桥&lt;code>br0&lt;/code>，随后便进入了&lt;code>br0&lt;/code>上的 OpenFlow 流表：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>table=0, n_packets=830232155588, n_bytes=328613498734351, priority=100,ip actions=goto_table:20&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=20, n_packets=1901, n_bytes=299279, priority=100,ip,in_port=6635,nw_src=10.130.9.154 actions=load:0-&amp;gt;NXM_NX_REG0[],goto_table:21&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=21, n_packets=834180030914, n_bytes=330064497351030, priority=0 actions=goto_table:30&lt;/code>&lt;/p>
&lt;p>与 Pod to Local Pod 的流程一致，数据包根据规则转到 table30；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70&lt;/code>&lt;/p>
&lt;p>数据包的目的地址为 Pod2 IP 10.131.8.206，不属于本节点 Pod 的 CIDR 网段 10.130.8.0/23，而属于集群 Pod 的 CIDR 网段 10.128.0.0/14，因此转到 table90；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=90, n_packets=15802525677, n_bytes=6091612778189, priority=100,ip,nw_dst=10.131.8.0/23 actions=move:NXM_NX_REG0[]-&amp;gt;NXM_NX_TUN_ID[0..31],set_field:10.122.28.8-&amp;gt;tun_dst,output:1&lt;/code>&lt;/p>
&lt;p>table90 根据目的 IP 的所属网段 10.131.8.0/23 判断其位于 Node2 上，于是将 Node2 IP 10.122.28.8 设置为&lt;code>tun_dst&lt;/code>。并且从寄存器 0 中取出 VNID 的值，从 1 号端口&lt;code>vxlan0&lt;/code>输出。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>vxlan0&lt;/code>作为一个 VTEP 设备（参见 &lt;a href="/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/sdn/#overlay-network">Overlay Network&lt;/a>），将根据 table90 发来的信息，对数据包进行一层封装：&lt;/p>
&lt;ul>
&lt;li>目的地址（dst IP） &amp;ndash;&amp;gt; &lt;code>tun_dst&lt;/code> &amp;ndash;&amp;gt; 10.122.28.8&lt;/li>
&lt;li>源地址（src IP） &amp;ndash;&amp;gt; Node1 IP &amp;ndash;&amp;gt; 10.122.28.7&lt;/li>
&lt;li>源 VNID &amp;ndash;&amp;gt; &lt;code>NXM_NX_TUN_ID[0..31]&lt;/code> &amp;ndash;&amp;gt; 0&lt;/li>
&lt;/ul>
&lt;p>由于封装后的数据包源/目的地址均为节点 IP，因此从 Node1 的网卡流出后，可以通过物理网络设备转发到 Node2 上。&lt;/p>
&lt;h3 id="packet-in-remote-pod">Packet in Remote Pod&lt;/h3>
&lt;p>Node2 上的&lt;code>vxlan0&lt;/code>对数据包进行解封，随后从&lt;code>br0&lt;/code>上的 1 号端口进入 OpenFlow 流表中：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>table=0, n_packets=52141153195, n_bytes=17269645342781, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.131.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]-&amp;gt;NXM_NX_REG0[],goto_table:10&lt;/code>&lt;/p>
&lt;p>table0 判断数据包的流入端口&lt;code>in_port&lt;/code>、源 IP 所属网段&lt;code>nw_src&lt;/code>和目的 IP 所属网段&lt;code>nw_dst&lt;/code>均符合该条规则，于是保存数据包中的源 VNID 到寄存器 0 后转到 table10；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=10, n_packets=10147760036, n_bytes=4060517391502, priority=100,tun_src=10.122.28.7 actions=goto_table:30&lt;/code>&lt;/p>
&lt;p>table10 确认 VxLAN 隧道的源 IP&lt;code>tun_src&lt;/code>就是节点 Node1 的 IP 地址，于是转到 table30；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=30, n_packets=678759566065, n_bytes=172831151192704, priority=200,ip,nw_dst=10.131.8.0/23 actions=goto_table:70&lt;/code>&lt;/p>
&lt;p>table30 确认数据包的目的 IP（即 Pod2 IP）存在于 Node2 中 Pod 的 CIDR 网段内，因此转到 table70；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=70, n_packets=193211683, n_bytes=27881218388, priority=100,ip,nw_dst=10.131.8.206 actions=load:0-&amp;gt;NXM_NX_REG1[],load:0x220-&amp;gt;NXM_NX_REG2[],goto_table:80&lt;/code>&lt;/p>
&lt;p>table70 发现数据包的目的 IP 与 Pod2 IP 相符，于是将 Pod2 的 VNID 作为目的 VNID 存于寄存器 1 中，将&lt;code>0x220&lt;/code>（十进制数 544）保存在寄存器 2 中，然后转到 table80；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=80, n_packets=676813794014, n_bytes=172576112594488, priority=200 actions=output:NXM_NX_REG2[]&lt;/code>&lt;/p>
&lt;p>table80 会检查保存在寄存器 0 和寄存器 1 中的源/目的 VNID，若相等（此例中均为 0），则从 544 号端口输出。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>br0&lt;/code>上的 554 端口对应的网络接口是&lt;code>vethe9f523a9&lt;/code>，因此数据包便最终通过它流入到了 Pod2 中。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">[&lt;/span>root@node2 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ovs-ofctl -O OpenFlow13 show br0 | grep 544&lt;/span>
544&lt;span class="o">(&lt;/span>vethe9f523a9&lt;span class="o">)&lt;/span>: addr:b2:a1:61:00:dc:3b
&lt;span class="o">[&lt;/span>root@node2 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ip a show vethe9f523a9&lt;/span>
559: vethe9f523a9@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu &lt;span class="m">1450&lt;/span> qdisc noqueue master ovs-system state UP
link/ether b2:a1:61:00:dc:3b brd ff:ff:ff:ff:ff:ff link-netnsid &lt;span class="m">54&lt;/span>
inet6 fe80::b0a1:61ff:fe00:dc3b/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="pod-to-service">Pod to Service&lt;/h2>
&lt;p>在本例中，Pod1 通过 Service 访问其后端的 Pod2，其 ClusterIP 为 172.30.107.57，监听的端口为 8080：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">[&lt;/span>root@node1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># oc get svc&lt;/span>
NAME CLUSTER-IP EXTERNAL-IP PORT&lt;span class="o">(&lt;/span>S&lt;span class="o">)&lt;/span> AGE
myService 172.30.107.57 &amp;lt;none&amp;gt; 8080/TCP 2y
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;ul>
&lt;li>
&lt;p>&lt;code>table=30, n_packets=21065939280, n_bytes=29573447694924, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60&lt;/code>&lt;/p>
&lt;p>数据包在送到 OpenFlow 流表 table30 前的步骤与 Pod to Local Pod 和 Pod to Remote Pod 中的情况一致，但数据包的目的地址变为了 myService 的 ClusterIP。因此将匹配&lt;code>nw_dst&lt;/code>中的 172.30.0.0/16 网段，转到 table60；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=60, n_packets=0, n_bytes=0, priority=100,tcp,nw_dst=172.30.107.57,tp_dst=8080 actions=load:0-&amp;gt;NXM_NX_REG1[],load:0x2-&amp;gt;NXM_NX_REG2[],goto_table:80&lt;/code>&lt;/p>
&lt;p>table60 匹配目的地址&lt;code>nw_dst&lt;/code>为 172.30.107.57 且目的端口为 8080 的数据包，并将 Pod1 的 VNID 0 保存到寄存器 1 中，将&lt;code>0x2&lt;/code>（十进制数字 2）保存到寄存器 2 中，转到 table80；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>table=80, n_packets=1113435014018, n_bytes=294106102133061, priority=200 actions=output:NXM_NX_REG2[]&lt;/code>&lt;/p>
&lt;p>table80 首先检查目的 Service 的 VNID 是否与寄存器 1 中的 VNID 一致，然后根据寄存器 2 中的数字将数据包从 2 号端口&lt;code>tun0&lt;/code>送出，最后进入节点的 iptables 规则中。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>iptables 对数据包的处理流程如下图所示：&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/20210516142844.png" alt="20210516142844">&lt;/p>
&lt;p>由于 Service 的实现依赖于 NAT（上图中的紫色方框），因此我们可以在 NAT 表中查看到与之相关的规则：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">[&lt;/span>root@node1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># iptables -t nat -nvL&lt;/span>
Chain OUTPUT &lt;span class="o">(&lt;/span>policy ACCEPT &lt;span class="m">4753&lt;/span> packets, 489K bytes&lt;span class="o">)&lt;/span>
pkts bytes target prot opt in out &lt;span class="nb">source&lt;/span> destination
2702M 274G KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */
Chain KUBE-SERVICES &lt;span class="o">(&lt;/span>&lt;span class="m">2&lt;/span> references&lt;span class="o">)&lt;/span>
pkts bytes target prot opt in out &lt;span class="nb">source&lt;/span> destination
&lt;span class="m">4&lt;/span> &lt;span class="m">240&lt;/span> KUBE-SVC-QYWOVDCBPMWAGC37 tcp -- * * 0.0.0.0/0 172.30.107.57 /* demo/myService:8080-8080 cluster IP */ tcp dpt:8080
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>本机产生的数据包（Locally-generated Packet）首先进入&lt;code>OUTPUT&lt;/code>链，然后匹配到自定义链&lt;code>KUBE-SERVICES&lt;/code>。由于其目的地址为 Service 的 ClusterIP 172.30.107.57，因此将再次跳转到对应的&lt;code>KUBE-SVC-QYWOVDCBPMWAGC37&lt;/code>链：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">Chain KUBE-SVC-QYWOVDCBPMWAGC37 &lt;span class="o">(&lt;/span>&lt;span class="m">1&lt;/span> references&lt;span class="o">)&lt;/span>
pkts bytes target prot opt in out &lt;span class="nb">source&lt;/span> destination
&lt;span class="m">1&lt;/span> &lt;span class="m">60&lt;/span> KUBE-SEP-AF5DIL6JV3XLLV6G all -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ statistic mode random probability 0.50000000000
&lt;span class="m">1&lt;/span> &lt;span class="m">60&lt;/span> KUBE-SEP-ADAJHSV7RYS5DUBX all -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */
Chain KUBE-SEP-ADAJHSV7RYS5DUBX &lt;span class="o">(&lt;/span>&lt;span class="m">1&lt;/span> references&lt;span class="o">)&lt;/span>
pkts bytes target prot opt in out &lt;span class="nb">source&lt;/span> destination
&lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> KUBE-MARK-MASQ all -- * * 10.131.8.206 0.0.0.0/0 /* demo/myService:8080-8080 */
&lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ tcp to:10.131.8.206:8080
Chain KUBE-SEP-AF5DIL6JV3XLLV6G &lt;span class="o">(&lt;/span>&lt;span class="m">1&lt;/span> references&lt;span class="o">)&lt;/span>
pkts bytes target prot opt in out &lt;span class="nb">source&lt;/span> destination
&lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> KUBE-MARK-MASQ all -- * * 10.128.10.57 0.0.0.0/0 /* demo/myService:8080-8080 */
&lt;span class="m">23&lt;/span> &lt;span class="m">1380&lt;/span> DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ tcp to:10.128.10.57:8080
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>KUBE-SVC-QYWOVDCBPMWAGC37&lt;/code>链下有两条完全相同的匹配规则，对应了该 Service 后端的两个 Pod。&lt;code>KUBE-SEP-ADAJHSV7RYS5DUBX&lt;/code>链和 &lt;code>KUBE-SEP-AF5DIL6JV3XLLV6G&lt;/code>链能够执行 DNAT 操作，分别将数据包的目的地址转化为 Pod IP 10.131.8.206 和 10.128.10.57。在一次通信中只会有一条链生效，这体现了 Service 的负载均衡能力。&lt;/p>
&lt;p>完成&lt;code>OUTPUT&lt;/code>DNAT 的数据包将进入节点的路由判断（Routing Decision）。由于当前目的地址已经属于集群内 Pod 的 CIDR 网段 10.128.0.0/14，因此将再次从&lt;code>tun0&lt;/code>端口再次进入 OVS 网桥&lt;code>br0&lt;/code>中。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;span class="lnt">9
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">[&lt;/span>rootnode1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># route -n&lt;/span>
Kernel IP routing table
Destination Gateway Genmask Flags Metric Ref Use Iface
0.0.0.0 10.122.28.1 0.0.0.0 UG &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> eth0
10.122.28.0 0.0.0.0 255.255.255.128 U &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> eth0
10.128.0.0 0.0.0.0 255.252.0.0 U &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> tun0
169.254.0.0 0.0.0.0 255.255.0.0 U &lt;span class="m">1008&lt;/span> &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> eth0
172.17.0.0 0.0.0.0 255.255.0.0 U &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> docker0
172.30.0.0 0.0.0.0 255.255.0.0 U &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> tun0
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>不过数据包在进入&lt;code>br0&lt;/code>之前，还需要经过 iptables 中的&lt;code>POSTROUTING&lt;/code>链，完成一次 MASQUERADE 操作：数据包的源地址转换为其流出端口的 IP，即&lt;code>tun0&lt;/code>的 IP 10.130.8.1。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">[&lt;/span>root@node1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># iptables -t nat -nvL &lt;/span>
Chain POSTROUTING &lt;span class="o">(&lt;/span>policy ACCEPT &lt;span class="m">5083&lt;/span> packets, 524K bytes&lt;span class="o">)&lt;/span>
pkts bytes target prot opt in out &lt;span class="nb">source&lt;/span> destination
2925M 288G OPENSHIFT-MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* rules &lt;span class="k">for&lt;/span> masquerading OpenShift traffic */
Chain OPENSHIFT-MASQUERADE &lt;span class="o">(&lt;/span>&lt;span class="m">1&lt;/span> references&lt;span class="o">)&lt;/span>
pkts bytes target prot opt in out &lt;span class="nb">source&lt;/span> destination
321M 19G MASQUERADE all -- * * 10.128.0.0/14 0.0.0.0/0 /* masquerade pod-to-service and pod-to-external traffic */
&lt;span class="o">[&lt;/span>root@node1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># ip a | grep tun0&lt;/span>
16: tun0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu &lt;span class="m">1450&lt;/span> qdisc noqueue state UNKNOWN qlen &lt;span class="m">1000&lt;/span>
inet 10.130.8.1/23 scope global tun0
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>本例中 Service 的后端 Pod 均在 Pod1 所在的节点外，因此数据包第二次进入 OpenFlow 流表时匹配的规则与 Pod to Remote Pod 一致。其传递流程如下图所示：&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132044.jpeg" alt="202205132044">&lt;/p>
&lt;p>Pod2 返回的数据包在到达 Node1 后将被&lt;code>vxlan0&lt;/code>解封装，然后根据其目的地址&lt;code>tun0&lt;/code>进入 OpenFlow 流表：&lt;/p>
&lt;ul>
&lt;li>&lt;code>table=0, n_packets=1084362760247, n_bytes=297224518823222, priority=200,ip,in_port=2 actions=goto_table:30&lt;/code>&lt;/li>
&lt;li>&lt;code>table=30, n_packets=20784385211, n_bytes=4742514750371, priority=300,ip,nw_dst=10.130.8.1 actions=output:2&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>数据包从 2 号端口&lt;code>tun0&lt;/code>流出后进入节点的 iptables 规则，随后将触发 iptables 的 &lt;a href="https://superuser.com/questions/1269859/linux-netfilter-how-does-connection-tracking-track-connections-changed-by-nat">Connection Tracking&lt;/a> 操作：根据 &lt;strong>/proc/net/nf_conntrack&lt;/strong> 文件中的记录进行“DeNAT”。返回数据包的源/目的地址从 Pod2 IP 10.131.8.206 和 tun0 IP 10.130.8.1，变回 Service 的 ClusterIP 172.30.107.57 和 Pod1 IP 10.130.9.154。&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="o">[&lt;/span>root@node1 ~&lt;span class="o">]&lt;/span>&lt;span class="c1"># cat /proc/net/nf_conntrack | grep -E &amp;#34;src=10.130.9.154.*dst=172.30.107.57.*dport=8080.*src=10.131.8.206&amp;#34;&lt;/span>
ipv4 &lt;span class="m">2&lt;/span> tcp &lt;span class="m">6&lt;/span> &lt;span class="m">431986&lt;/span> ESTABLISHED &lt;span class="nv">src&lt;/span>&lt;span class="o">=&lt;/span>10.130.9.154 &lt;span class="nv">dst&lt;/span>&lt;span class="o">=&lt;/span>172.30.107.57 &lt;span class="nv">sport&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">80&lt;/span> &lt;span class="nv">dport&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">8080&lt;/span> &lt;span class="nv">src&lt;/span>&lt;span class="o">=&lt;/span>10.131.8.206 &lt;span class="nv">dst&lt;/span>&lt;span class="o">=&lt;/span>10.130.8.1 &lt;span class="nv">sport&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">8080&lt;/span> &lt;span class="nv">dport&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">80&lt;/span> &lt;span class="o">[&lt;/span>ASSURED&lt;span class="o">]&lt;/span> &lt;span class="nv">mark&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">0&lt;/span> &lt;span class="nv">secctx&lt;/span>&lt;span class="o">=&lt;/span>system_u:object_r:unlabeled_t:s0 &lt;span class="nv">zone&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">0&lt;/span> &lt;span class="nv">use&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">2&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="pod-to-external">Pod to External&lt;/h2>
&lt;p>数据包依然首先通过&lt;code>veth-pair&lt;/code>送往 OVS 网桥&lt;code>br0&lt;/code>，随后便进入了&lt;code>br0&lt;/code>上的 OpenFlow 流表：&lt;/p>
&lt;ul>
&lt;li>&lt;code>table=0, n_packets=837268653828, n_bytes=331648403594327, priority=100,ip actions=goto_table:20&lt;/code>&lt;/li>
&lt;li>&lt;code>table=20, n_packets=613807687, n_bytes=220557571042, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0-&amp;gt;NXM_NX_REG0[],goto_table:21&lt;/code>&lt;/li>
&lt;li>&lt;code>table=21, n_packets=837674296060, n_bytes=331665441915651, priority=0 actions=goto_table:30&lt;/code>&lt;/li>
&lt;li>&lt;code>table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100&lt;/code>&lt;/li>
&lt;li>&lt;code>table=100, n_packets=761732023982, n_bytes=282091648536325, priority=0 actions=output:2&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>数据包从&lt;code>tun0&lt;/code>端口流出后进入节点的路由表及 iptables 规则：&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">Chain POSTROUTING &lt;span class="o">(&lt;/span>policy ACCEPT &lt;span class="m">2910&lt;/span> packets, 299K bytes&lt;span class="o">)&lt;/span>
pkts bytes target prot opt in out &lt;span class="nb">source&lt;/span> destination
&lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0
2940M 289G OPENSHIFT-MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* rules &lt;span class="k">for&lt;/span> masquerading OpenShift traffic */
Chain OPENSHIFT-MASQUERADE &lt;span class="o">(&lt;/span>&lt;span class="m">1&lt;/span> references&lt;span class="o">)&lt;/span>
pkts bytes target prot opt in out &lt;span class="nb">source&lt;/span> destination
322M 19G MASQUERADE all -- * * 10.128.0.0/14 0.0.0.0/0 /* masquerade pod-to-service and pod-to-external traffic */
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>访问集群外部显然需要通过节点的默认网关，因此数据包将从节点网卡&lt;code>eth0&lt;/code>送出。而在&lt;code>POSTROUTING&lt;/code>链中，数据包的源地址由 Pod IP 转换为了&lt;code>eth0&lt;/code>的 IP 10.122.28.7。完整流程如下图所示：&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132045.jpeg" alt="202205132045">&lt;/p>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>&lt;a href="https://medoc.readthedocs.io/en/latest/docs/ovs/sharing/cloud_usage.html">OVS 在云项目中的使用&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html">OpenShift SDN - Networking | Architecture | OpenShift Container Platform 3.11&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.cnblogs.com/sammyliu/p/10064450.html">理解 OpenShift（3）：网络之 SDN&lt;/a>&lt;/p></content><category scheme="/tags/kubernetes/" term="Kubernetes" label="Kubernetes"/><category scheme="/tags/openshift/" term="Openshift" label="Openshift"/><category scheme="/tags/network/" term="Network" label="Network"/><category scheme="/tags/cni/" term="CNI" label="CNI"/><category scheme="/tags/open-vswitch/" term="Open vSwitch" label="Open vSwitch"/></entry><entry><title type="text">Kubernetes 中的 SDN 网络模型</title><link rel="alternate" type="text/html" href="/posts/kubernetes-sdn/"/><id>/posts/kubernetes-sdn/</id><updated>2021-10-25T22:18:43+08:00</updated><published>2020-05-09T09:19:42+01:00</published><author><name>koktlzz</name><uri>/</uri><email>koktlgwr@gmail.com</email></author><rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">前言 A Guide to the Kubernetes Networking Model 一文生动形象地介绍了 Kubernetes 中的网络模型，然而受篇幅所限，作者并没有对 Pod 跨节点通信时数据包在节点之间传递的细节进行过多讨论。
我们已经知道，Docker 使用端口映射的方式实现不同主机间容器的通信，Kubernetes 中同样也有 hostPort 的概念。但是当节点和 Pod 的数量上升后，手动管理节点上绑定的端口是十分困难的，这也是NodePort类型的 Service 的缺点之一。而一旦 Pod 不再“借用”节点的 IP 和端口来暴露自身的服务，就不得不面临一个棘手的问题：Pod 的本质是节点中的进程，节点外的物理网络设备（交换机/路由器）并不知晓 Pod 的存在。它们在接收目的地址为 Pod IP 的数据包时，无法完成进一步的传输工作。</summary><content type="html">&lt;h2 id="前言">前言&lt;/h2>
&lt;p>&lt;a href="https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/">A Guide to the Kubernetes Networking Model&lt;/a> 一文生动形象地介绍了 Kubernetes 中的网络模型，然而受篇幅所限，作者并没有对 Pod 跨节点通信时数据包在节点之间传递的细节进行过多讨论。&lt;/p>
&lt;p>我们已经知道，Docker 使用端口映射的方式实现不同主机间容器的通信，Kubernetes 中同样也有 &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#pod-v1-core">&lt;code>hostPort&lt;/code>&lt;/a> 的概念。但是当节点和 Pod 的数量上升后，手动管理节点上绑定的端口是十分困难的，这也是&lt;code>NodePort&lt;/code>类型的 Service 的缺点之一。而一旦 Pod 不再“借用”节点的 IP 和端口来暴露自身的服务，就不得不面临一个棘手的问题：Pod 的本质是节点中的进程，节点外的物理网络设备（交换机/路由器）并不知晓 Pod 的存在。它们在接收目的地址为 Pod IP 的数据包时，无法完成进一步的传输工作。&lt;/p>
&lt;p>为此我们需要使用一些 CNI（Container Network Interface）插件（如 Flannel、Calico 和 Open vSwitch 等）来优化 Kubernetes 集群的网络模型，这种新型的网络设计理念称为 SDN（Software-defined Networking）。根据 SDN 实现的层级，我们可以将其分为 Underlay Network 和 Overlay Network：&lt;/p>
&lt;blockquote>
&lt;p>Overlay 网络允许设备跨越底层物理网络（Underlay Network）进行通信，而底层却并不知晓 Overlay 网络的存在。Underlay 网络是专门用来承载用户 IP 流量的基础架构层，它与 Overlay 网络之间的关系有点类似物理机和虚拟机。Underlay 网络和物理机都是真正存在的实体，它们分别对应着真实存在的网络设备和计算设备，而 Overlay 网络和虚拟机都是依托在下层实体使用软件虚拟出来的层级。&lt;/p>
&lt;/blockquote>
&lt;h2 id="underlay-network">Underlay Network&lt;/h2>
&lt;p>利用 Underlay Network 实现 Pod 跨节点通信，既可以只依赖 TCP/IP 模型中的二层协议，也可以使用三层。但无论哪种实现方式，都必须对底层的物理网络有所要求。&lt;/p>
&lt;h3 id="二层">二层&lt;/h3>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202105072039.jpeg" alt="202105072039">&lt;/p>
&lt;p>如图所示，Pod 与节点的 IP 地址均处于同一网段。当 Pod1 向另一节点上的 Pod2 发起通信时，数据包首先通过&lt;code>veth-pair&lt;/code>和&lt;code>cbr0&lt;/code>送往 Node1 的网卡。由于目的地址 10.86.44.4 与 Node1 同网段，因此 Node1 将通过 ARP 广播请求 10.86.44.4 的 MAC 地址。&lt;/p>
&lt;p>CNI 插件不仅为 Pod 分配 IP 地址，它还会将每个 Pod 所在的节点信息下发给 SDN 交换机。这样当 SDN 交换机接收到 ARP 请求时，将会答复 Pod2 所在节点 Node2 的 MAC 地址，数据包也就顺利地送到了 Node2 上。&lt;/p>
&lt;h3 id="三层">三层&lt;/h3>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202105080143.jpeg" alt="202105080143">&lt;/p>
&lt;p>如图所示，Pod 与节点的 IP 地址不再处于同一网段。当 Pod1 向另一节点上的 Pod2 发起通信时，数据包首先通过&lt;code>veth-pair&lt;/code>和&lt;code>cbr0&lt;/code>进入宿主机内核的路由表（Routing Table）。CNI 插件在该表中添加了若干条路由规则，如目的地址为 Pod2 IP 的网关为 Node2 的 IP。这样数据包的目的 MAC 地址就变为了 Node2 的 MAC 地址，它将会通过交换机发送到 Node2 上。&lt;/p>
&lt;p>由于这种实现方式基于三层协议，因此不要求 Node1 和 Node2 处于同一网段。如下图所示，此时需要将目的地址为 Pod2 IP 的网关设置为路由器的 IP。数据包的目的 MAC 地址首先变为路由器的 MAC 地址，然后经过路由器后再变为 Node2 的 MAC 地址。&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205080146.jpeg" alt="202205080146">&lt;/p>
&lt;p>通过上面的描述我们可以发现，想要实现三层的 Underlay 网络，需要在多个节点间下发和同步路由表。于是很容易想到用于交换路由信息的 BGP（Border Gateway Protocol）协议：&lt;/p>
&lt;blockquote>
&lt;p>边界网关协议（英语：Border Gateway Protocol，缩写：BGP）是互联网上一个核心的去中心化自治路由协议。它通过维护 IP 路由表或“前缀”表来实现自治系统（AS）之间的可达性，属于矢量路由协议。BGP 不使用传统的内部网关协议（IGP）的指标，而使用基于路径、网络策略或规则集来决定路由。因此，它更适合被称为矢量性协议，而不是路由协议。&lt;/p>
&lt;/blockquote>
&lt;p>对于 Calico 的 BGP 模式来说，我们可以把集群网络模型视为在每个节点上都部署了一台虚拟路由器。路由器可以与其他节点上的路由器通过 BGP 协议互通，它们称为一对 BGP Peers。Calico 的默认部署方式为 Full-mesh，即创建一个完整的内部 BGP 连接网，每个节点上的路由器均互为 BGP Peers。这种方式仅适用于 100 个节点以内的中小型集群，在大型集群中使用的效率低下。而 Route reflectors 模式则将部分节点作为路由反射器，其他节点上的路由器只需与路由反射器互为 BGP Peers。这样便可以大大减少集群中 BGP Peers 的数量，从而提升效率。&lt;/p>
&lt;h2 id="overlay-network">Overlay Network&lt;/h2>
&lt;p>Overlay 网络可以通过多种协议实现，但通常是对 IP 数据包进行一层外部封装（Encapsulation）。这样底层的 Underlay 网络便只会看到外部封装的数据，而无需处理内部的原有数据。Overlay 网络发送数据包的方式取决于其类型和使用的协议，如基于 VxLAN 实现 Overlay 网络，数据包将被外部封装后以 UDP 协议进行发送：&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/NoteImg@main/20210508103310.png" alt="20210508103310">&lt;/p>
&lt;p>Overlay 网络的实现并不依赖于底层物理网络设备，因此我们就以一个两节点不处于同一网段且 Pod 与节点亦处于不同网段的例子来说明 Overlay 网络中的数据包传递过程。集群网络使用 VxLAN 技术组建，虚拟网络设备 VTEP（Virtual Tunnel End Point）将会完成数据包的封装和解封操作。&lt;/p>
&lt;p>&lt;img src="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202105080221.jpeg" alt="202105080221">&lt;/p>
&lt;p>Node1 上的 VTEP 收到 Pod1 发来的数据包后，首先会在本地的转发表中查找目的 Pod 所在节点的 IP，即 192.168.1.100。随后它将本机 IP 地址 10.86.44.2、Node2 的 IP 地址 192.168.1.100 和 Pod1 的 VNID（VxLAN Network Identifier）封装在原始数据包外，从 Node1 的网络接口 eth0 送出。由于新构建的数据包源/目的地址均为节点的 IP，因此外部的路由器可以将其转发到 Node2 上。Node2 中的 VTEP 在接收到数据包后会首先进行解封，若源 VNID（Pod1 的 VNID）与目的 VNID（Pod2 的 VNID）一致，便会根据原始数据包中的目的地址 172.100.1.2 将其发送到 Pod2 上。此处的 VNID 检查，主要是为了实现集群的网络策略管理和多租户隔离。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>通过对上述几种 SDN 网络模型的讨论，我们可以发现只有 Overlay 网络需要对数据包进行封装和解封，因此它的性能相比于 Underlay 网络较差。但 Overlay 网络也有以下优点：&lt;/p>
&lt;ul>
&lt;li>对底层网络设备的依赖性最小。即使 Pod 所在的节点发生迁移，依然可以通过 Overlay 网络与原集群实现二层网络的互通；&lt;/li>
&lt;li>VNID 共有 24 位，因此可以构造出约 1600 万个互相隔离的虚拟网络。&lt;/li>
&lt;/ul>
&lt;h2 id="参考文献">参考文献&lt;/h2>
&lt;p>&lt;a href="https://docs.projectcalico.org/about/about-kubernetes-networking">About Kubernetes Networking&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://zh.wikipedia.org/wiki/%E8%BE%B9%E7%95%8C%E7%BD%91%E5%85%B3%E5%8D%8F%E8%AE%AE">边界网关协议&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://docs.projectcalico.org/networking/bgp">Configure BGP peering&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://draveness.me/whys-the-design-overlay-network/">为什么集群需要 Overlay 网络&lt;/a>&lt;/p></content><category scheme="/tags/kubernetes/" term="Kubernetes" label="Kubernetes"/><category scheme="/tags/network/" term="Network" label="Network"/><category scheme="/tags/cni/" term="CNI" label="CNI"/></entry></feed>